{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"E4_Lyricist.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1m_-BFXPZc8tUKwtk226xsv9MiOKc338J","authorship_tag":"ABX9TyNNTO1b7I0dsQp8TKsitOSv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zXckCEdVTggc"},"source":["# E4. 작사가 인공지능 만들기\n","- 순환신경망(RNN)의 LSTM 모델을 활용해서 시작 단어(들) input하면 노래 가사를 output해주는 인공지능을 만듭니다.\n","- 텐서플로우에서 제공하는 모델을 활용할 것이며, 학습데이터는 영어 노래가사 데이터를 변환한 124981개의 텐서입니다."]},{"cell_type":"markdown","metadata":{"id":"VcCd6LL1T7-c"},"source":["**실습목차**\n","\n","1. 텍스트 데이터 읽어오기\n","2. 텍스트 데이터 정제하기\n","3. 작사가 인공지능 만들기   \n","    3-1) lyricist 모델 만들고 학습하기    \n","    3-2) lyricist 모델 평가하기\n","4. 회고\n"]},{"cell_type":"markdown","metadata":{"id":"UeJjpef1UDxh"},"source":["**실습 개요**\n","- 이번에는 자연어 프로세싱을 연습해볼 수 있는 실습입니다. NLP(Natural Language Processing)의 대표적인 모델로 Recurrent Neural Networks(RNN, 순환신경망)이 있습니다. 관련해서 ratsgo's blog 내용을 인용합니다. RNN은 \"히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는(directed cycle) 인공신경망의 한 종류입니다. 음성, 문자 등 순차적으로 등장하는 데이터 처리에 적합한 모델로 알려져 있는데요. ... 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조이기 때문에 필요에 따라 다양하고 유연하게 구조를 만들 수 있다는 점이 RNN의 가장 큰 장점입니다.\"(https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)\n","- RNN의 vanishing gradient problem에 대안으로 LSTM 모델을 사용합니다. 위의 ratsgo's blog 글에서 인용하면, \"RNN은 관련 정보와 그 정보를 사용하는 지점 사이 거리가 멀 경우 역전파시 그래디언트가 점차 줄어 학습능력이 크게 저하되는 것으로 알려져 있습니다. 이를 vanishing gradient problem이라고 합니다. ... LSTM은 RNN의 히든 state에 cell-state를 추가한 구조입니다.\"\n","- 실습에서는 RNN의 일종으로 RNN을 보완한 Long Short-Term Memory models(LSTM) 모델을 활용할 것이며, 아래 그림과 같이 4개 레이어(1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어)로 구성을 할 것입니다.\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1i1JfB3tEs5SBE8Y78Hfo2tw5cAPXDIMW\" width=\"500\" />\n","(출처: AIFFEL LMS Exploration 4)   \n","\n","<br>   \n","    \n","**Insight**\n","- 2020년 중순에 나온 OpenAI의 GPT-3 는 사람이 쓴 것과 비교해도 손색 없을만한 필력으로 기사, 소설, 심지어는 프로그래밍 언어까지 어떤 글감이든 AI가 생성하며 주목을 받았습니다. 또한 GPT-3는 실용성이 높으며, 이전 버전인 GPT-2와 비교하면 알고리즘은 크게 다르지 않지만 학습 규모가 어마어마한 대규모 언어 모델로 평가 받습니다. 이에 대해 MIT Technology Review는 \"일례로, GPT-2 신경망의 파라미터는 15억 개였지만 GPT-3 신경망의 파라미터는 1750억 개다. 학습 데이터의 경우도 마찬가지다. 비교 자체가 힘겨운 수준이다.\" (2021년, MIT Technology Review, https://www.technologyreview.kr/gpt3-best-worst-ai-openai-natural-language/)\n","- 위 기사는 이렇게 마무리됩니다. \"GPT-3는 딥 러닝의 현주소이자, AI의 명암이 모두 담긴 소우주다.\"\n","- 이번 실습에서 작문을 하는 딥러닝 모델을 구현해보는 것은 최근 딥러닝 혹은 AI의 현주소를 살펴보는 시작점이라고 생각합니다. 이미 GPT-3를 통해서는 기계와 인간이 자연어로 꽤 원활한 의사소통이 가능해졌습니다. 정리하면, 기계의 자연어 처리는 기계가 사람의 언어를 이해해서 사람과 같이 반응하고 표현하는데 필수적입니다."]},{"cell_type":"markdown","metadata":{"id":"IuelM8gDp-MM"},"source":["## 1. 텍스트 데이터 읽어오기"]},{"cell_type":"markdown","metadata":{"id":"ZAMVcoXnmkan"},"source":["- 먼저, 영어 노래가사 텍스트 데이터가 담긴 파일들을 읽어올 것입니다."]},{"cell_type":"code","metadata":{"id":"GAOeewESTUbv"},"source":["# 필요한 모듈을 가져옵니다.\n","import glob, os, re \n","import numpy as np\n","import tensorflow as tf\n","import sklearn\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xI8wOi_CnMOZ","executionInfo":{"status":"ok","timestamp":1627349530501,"user_tz":-540,"elapsed":13913,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"c876f261-ddd7-4a64-d50b-9e72ee14e32f"},"source":["# 구글 코랩에서 구글 드라이브 저장위치의 노래가사 텍스트 파일들을 읽어옵니다.\n","txt_list = glob.glob('drive/MyDrive/Colab Notebooks/lyrics_data/*')\n","\n","raw_corpus = []\n","\n","# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n","for txt_file in txt_list:\n","    with open(txt_file, \"r\") as f:\n","        raw = f.read().splitlines()    # txt파일을 줄 단위로 끊어서 읽어옵니다.\n","        raw_corpus.extend(raw)\n","\n","print(\"데이터 크기:\", len(raw_corpus))    # raw_corpus에 담겨있는 요소들의 개수를 출력합니다.\n","print(\"Examples:\\n\", raw_corpus[:3])   # raw_corpus 를 앞에서부터 3라인 출력합니다."],"execution_count":null,"outputs":[{"output_type":"stream","text":["데이터 크기: 187088\n","Examples:\n"," [\"Let's stay together I, I'm I'm so in love with you\", 'Whatever you want to do', 'Is all right with me']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"czPYVshPzcHC"},"source":["## 2. 텍스트 데이터 정제하기"]},{"cell_type":"markdown","metadata":{"id":"gyYV6Dz2msrW"},"source":["- 텍스트 데이터를 그대로 읽어온 raw한 데이터는 토큰화에 적합하지 않기 때문에 정제해주어야 합니다. 정제할 때에는 정규표현식을 사용해서 텍스트를 원하는 형태로 가공해줍니다.\n","- 토큰화를 진행하면 12000자의 단어에 대응하는 사전이 생성되고, 이를 활용해 텐서로 변형해서 기계가 이해할 수 있는 텐서가 됩니다.\n","- 문장의 시작과 끝을 알리는 \\<start\\> 와 \\<end\\> 를 넣어줍니다. 훈련 데이터에서 소스 문장(input, \\<start\\>로 시작)과 타겟문장(\\<start\\>로 시작하지 않는 output 문장)을 만들어줄 때 필요합니다."]},{"cell_type":"code","metadata":{"id":"QwFQm__uteiA"},"source":["# 입력된 문장을 정규표현식을 활용해 정제해줍니다.\n","# 문장부호, 특수문자, 대소문자 등을 필터링하고, 후에 문장을 쪼개서 토큰화해줄 것입니다.\n","#        1. 소문자로 바꾸고, 양쪽 공백을 지우기\n","#        2. 특수문자 양쪽에 공백을 넣고\n","#        3. 여러개의 공백은 하나의 공백으로 바꿉니다\n","#        4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n","#        5. 다시 양쪽 공백을 지웁니다\n","#        6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip() # 1\n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n","    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n","    sentence = sentence.strip() # 5\n","    sentence = '<start> ' + sentence + ' <end>' # 6\n","    return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dsCI0Yr_gBqT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627349532711,"user_tz":-540,"elapsed":2215,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"7ca2276f-0279-4f64-f4cf-51d2783ca6d6"},"source":["# 정제된 문장을 모아줍니다.\n","corpus = []\n","\n","# raw_corpus 의 문장들을 정제해서 corpus 에 담아줍니다.\n","for sentence in raw_corpus:\n","    if len(sentence) == 0: continue     # 빈 공백 문장은 건너뜁니다.\n","\n","    preprocessed_sentence = preprocess_sentence(sentence)\n","    if len(re.split(r'\\s', preprocessed_sentence)) > 15: continue   # 문장에서 토큰 개수가 15개를 넘어가면 건너뜁니다.\n","\n","    corpus.append(preprocessed_sentence)\n","\n","corpus[:10]   # corpus 처음 10개를 출력해봅니다."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> whatever you want to do <end>',\n"," '<start> is all right with me <end>',\n"," '<start> cause you make me feel so brand new <end>',\n"," '<start> loving you forever <end>',\n"," '<start> is what i need <end>',\n"," '<start> let me , be the one you come running to <end>',\n"," '<start> i ll never be untrue oh baby <end>',\n"," '<start> let s , let s stay together gether <end>',\n"," '<start> lovin you whether , whether <end>',\n"," '<start> times are good or bad , happy or sad <end>']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"nZM8eYPFgB3n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627353630975,"user_tz":-540,"elapsed":3460,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"1601d06e-7cb6-4d62-9c59-0aecb6ededa9"},"source":["# 텐서플로 패키지를 사용해서 문장을 토큰화하고, 12,000자 단어사전을 만들어주며, 데이터를 숫자로 변환해 텐서를 반환합니다.\n","def tokenize(corpus):\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words = 12000,\n","        filters = ' ',\n","        oov_token=\"<unk>\"\n","    )\n","    tokenizer.fit_on_texts(corpus)\n","    tensor = tokenizer.texts_to_sequences(corpus)\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n","    \n","    print(\"문장 데이터를 텐서로 변환한 결과:\\n\", tensor, tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["문장 데이터를 텐서로 변환한 결과:\n"," [[   2  570    7 ...    0    0    0]\n"," [   2   26   25 ...    0    0    0]\n"," [   2   66    7 ...    0    0    0]\n"," ...\n"," [   2   20  149 ...    0    0    0]\n"," [   2    4   35 ...    3    0    0]\n"," [   2 1063   10 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f13d202f590>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vGJww1aXxqy1"},"source":["- tf.keras.preprocessing.text.Tokenizer 패키지는 정제된 데이터를 토큰화하고, 단어 사전(vocabulary 또는 dictionary라고 칭함)을 만들어주며, 데이터를 숫자로 변환까지 한 방에 해줍니다. 이 과정을 벡터화(vectorize) 라 하며, 숫자로 변환된 데이터를 텐서(tensor) 라고 칭합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAWRrdxAvPk4","executionInfo":{"status":"ok","timestamp":1627349536341,"user_tz":-540,"elapsed":25,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"0630ac71-4cdf-4657-f31a-e1915318b224"},"source":["# 생성된 토큰을 앞에서부터 10개 출력합니다.\n","for idx in tokenizer.index_word:\n","    print(idx, \":\", tokenizer.index_word[idx])\n","    \n","    if idx >= 10 : break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : i\n","5 : ,\n","6 : the\n","7 : you\n","8 : and\n","9 : a\n","10 : to\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuhv_-FBvPp_","executionInfo":{"status":"ok","timestamp":1627354092088,"user_tz":-540,"elapsed":266,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"1cf0543b-a319-46ad-a9e3-dec206c08b0c"},"source":["# 만들어진 tensor 에서 랜덤으로 5개 행을 출력해봅니다.\n","example_tensor = tensor[100:105, :]\n","print(example_tensor, \"\\n\")\n","\n","# tokenizer를 이용해 word index를 단어로 하나씩 변환합니다.\n","for idx, sentence in enumerate(example_tensor):\n","  print(\"{}번째 문장:\".format(idx+1), end=\" \")\n","  for word_index in sentence:\n","    if word_index == 0 : continue       # padding 해준 0은 단어장에 들어가있지 않으므로 건너뜁니다.\n","    print(tokenizer.index_word[word_index], end=\" \")\n","  print(\"\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[   2   71   11  179    5   71   11  276    5   49    3    0    0    0\n","     0]\n"," [   2   71   89 5748    3    0    0    0    0    0    0    0    0    0\n","     0]\n"," [   2 2972   28   36    5   71    5 1504    3    0    0    0    0    0\n","     0]\n"," [   2 2972   28   36    5 1504    3    0    0    0    0    0    0    0\n","     0]\n"," [   2 2972   28   36    3    0    0    0    0    0    0    0    0    0\n","     0]] \n","\n","1번째 문장: <start> say it again , say it together , yeah <end> \n","\n","2번째 문장: <start> say wanna moans <end> \n","\n","3번째 문장: <start> moan for love , say , hmm <end> \n","\n","4번째 문장: <start> moan for love , hmm <end> \n","\n","5번째 문장: <start> moan for love <end> \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pv8JrstgvPyV"},"source":["- 텐서는 \"2\" \\<start\\> 토큰으로 시작해서 \"3\" \\<end\\> 토큰 또는 \"0\" padding 토큰으로 끝납니다."]},{"cell_type":"code","metadata":{"id":"KgpdkCABvP6E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627355739512,"user_tz":-540,"elapsed":268,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"859ff365-41ab-490f-ddab-a1192900d2d6"},"source":["src_input = tensor[:,:-1]    # 모델의 입력이 되는 소스 문장\n","tgt_input = tensor[:,1:]     # <start>를 없애준 모델의 출력이 되는 타겟 문장\n","print(\"소스문장: \", src_input[0])\n","print(\"타겟문장: \", tgt_input[0])    # 소스문장과 비교하면 왼쪽으로 한칸 밀려서 2로 시작하지 않고 570으로 시작합니다.\n","\n","enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n","                                                          tgt_input, test_size = 0.2)   # 훈련 데이터, 테스트 데이터 분리"],"execution_count":null,"outputs":[{"output_type":"stream","text":["소스문장:  [  2 570   7  64  10  48   3   0   0   0   0   0   0   0]\n","타겟문장:  [570   7  64  10  48   3   0   0   0   0   0   0   0   0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"crX7NF2PvP79","executionInfo":{"status":"ok","timestamp":1627354576352,"user_tz":-540,"elapsed":306,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"62817620-d926-41e8-b391-fd26360dd818"},"source":["print(\"Source Train:\", enc_train.shape)\n","print(\"Target Train:\", dec_train.shape)\n","print(\"Source Test: \", enc_val.shape)\n","print(\"Target Test: \", dec_val.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Source Train: (124981, 14)\n","Target Train: (124981, 14)\n","Source Test:  (31246, 14)\n","Target Test:  (31246, 14)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pc31a30HvQBR"},"source":["## 3. 작사가 인공지능 만들기"]},{"cell_type":"markdown","metadata":{"id":"m-PBJYG4nnUP"},"source":["- LSTM 모델을 활용해서 작사를 해주는 인공지능을 만들어봅니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hhn2eLX1vQXW","executionInfo":{"status":"ok","timestamp":1627349542210,"user_tz":-540,"elapsed":295,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"b9a69713-0a0a-498a-d289-427f8e7c5fd8"},"source":["BUFFER_SIZE = len(enc_train)\n","BATCH_SIZE = 256\n","steps_per_epoch = len(enc_train) // BATCH_SIZE\n","\n","# tokenizer가 구축한 단어사전 내 12,000개와 여기 포함되지 않은 0:<pad>를 포함하여 12,001개\n","VOCAB_SIZE = tokenizer.num_words + 1\n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"1a4_c7m4CzeF"},"source":["### 3-1) lyricist 모델을 만들고 학습하기"]},{"cell_type":"code","metadata":{"id":"ZyZ1XWeovQat"},"source":["# Embedding, LSTM(1&2), Dense Layer 모델\n","# 1. Embedding 레이어는 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔 줍니다. \n","# 2,3. LSTM 레이어(1&2)에는 hidden_size(하이퍼파라미터)를 설정해줍니다.\n","# 4. Dense 레이어\n","\n","class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","        \n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)   # 1\n","        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)    # 2\n","        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)    # 3\n","        self.linear = tf.keras.layers.Dense(vocab_size)    # 4\n","        \n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.linear(out)\n","        \n","        return out\n","    \n","embedding_size = 256    # embedding_size 는 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기입니다.\n","                        # 크기가 클수록 추상적인 특징을 잘 잡아내지만 그만큼 데이터가 많이 필요합니다.\n","hidden_size = 1024\n","lyricist = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rpqv1L6QGpe5","executionInfo":{"status":"ok","timestamp":1627350809719,"user_tz":-540,"elapsed":390,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"7029cd6c-2cd6-4fcc-8264-cdf32cd8afa0"},"source":["for src_sample, tgt_sample in dataset.take(1): break\n","\n","# 한 배치만 불러온 데이터를 모델에 넣어봅니다.\n","example_batch_predictions = lyricist(src_sample)\n","\n","print(\"예측 결과의 shape:\", example_batch_predictions.shape, \n","                         \"  # (batch_size, sequence_length, vocab_size)\")\n","\n","example_batch_loss = loss(tgt_sample, example_batch_predictions)\n","mean_loss = example_batch_loss.numpy().mean()\n","print(\"Mean loss:      \", mean_loss)\n","\n","print(\"\\nouput tensor:\", example_batch_predictions)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["예측 결과의 shape: (256, 14, 12001)   # (batch_size, sequence_length, vocab_size)\n","Mean loss:       2.1477234\n","\n","ouput tensor: tf.Tensor(\n","[[[ -5.2156773    2.7802072  -11.153991   ...  -9.2401705  -11.680378\n","   -11.321727  ]\n","  [ -2.5218778    4.7929296  -11.689121   ... -15.687222   -11.2067585\n","   -11.841417  ]\n","  [ -7.34187      1.3228133  -15.271979   ...  -8.268049   -14.882353\n","   -15.253975  ]\n","  ...\n","  [ 10.653149    -6.951487   -20.19778    ... -42.327614   -37.449375\n","   -20.485767  ]\n","  [ 10.653712    -6.978196   -20.193573   ... -42.306793   -37.475372\n","   -20.48275   ]\n","  [ 10.659117    -6.9965205  -20.18801    ... -42.306656   -37.49851\n","   -20.47844   ]]\n","\n"," [[ -5.2156773    2.7802072  -11.153991   ...  -9.2401705  -11.680378\n","   -11.321727  ]\n","  [ -3.324497     2.599378   -13.593562   ... -14.724111   -12.290602\n","   -13.7701235 ]\n","  [ -5.5122914    0.77567613 -15.097858   ... -10.590948   -12.47462\n","   -15.20001   ]\n","  ...\n","  [ 10.57545     -6.4748673  -20.496452   ... -42.40565    -38.46872\n","   -20.72704   ]\n","  [ 10.675772    -6.6707897  -20.401575   ... -42.15025    -38.59538\n","   -20.658562  ]\n","  [ 10.647782    -6.7627354  -20.3244     ... -41.831123   -38.53482\n","   -20.607395  ]]\n","\n"," [[ -5.2156773    2.7802072  -11.153991   ...  -9.2401705  -11.680378\n","   -11.321727  ]\n","  [ -5.8477845    2.821193   -15.239446   ... -20.242891   -13.171881\n","   -15.202238  ]\n","  [ -6.5061865    3.0414257  -16.345222   ... -15.882644    -8.935798\n","   -16.67526   ]\n","  ...\n","  [ 10.615661    -7.103899   -20.23006    ... -42.40998    -37.969208\n","   -20.528185  ]\n","  [ 10.6737      -7.1340537  -20.295519   ... -42.459576   -38.180798\n","   -20.589556  ]\n","  [ 10.659688    -7.1530137  -20.306864   ... -42.37438    -38.208225\n","   -20.594704  ]]\n","\n"," ...\n","\n"," [[ -5.2156773    2.7802072  -11.153991   ...  -9.2401705  -11.680378\n","   -11.321727  ]\n","  [ -5.31166      1.3445079  -16.331614   ... -26.281197   -18.332478\n","   -16.410189  ]\n","  [ -5.2900486    2.1364236  -15.349956   ... -24.624464   -17.223732\n","   -15.410142  ]\n","  ...\n","  [ -5.9306583   -0.73093534 -16.709953   ...  -6.9164405  -25.75824\n","   -16.890324  ]\n","  [  8.171339    -5.751954   -20.960512   ... -39.522278   -41.31546\n","   -21.36427   ]\n","  [  9.915684    -5.604973   -20.327148   ... -40.22153    -39.521404\n","   -20.633379  ]]\n","\n"," [[ -5.2156773    2.7802072  -11.153991   ...  -9.2401705  -11.680378\n","   -11.321727  ]\n","  [ -5.364003     2.3506186  -15.840344   ... -23.349392   -15.189402\n","   -15.913401  ]\n","  [ -4.4679995    2.6435573  -15.520075   ... -21.703558   -16.161573\n","   -15.616863  ]\n","  ...\n","  [  9.722372    -6.4574323  -19.672667   ... -39.747364   -37.864235\n","   -19.91892   ]\n","  [ 10.624569    -6.9487214  -20.304749   ... -41.201965   -39.27643\n","   -20.520023  ]\n","  [ 10.784767    -7.077185   -20.263922   ... -41.564323   -39.419975\n","   -20.504747  ]]\n","\n"," [[ -5.2156773    2.7802072  -11.153991   ...  -9.2401705  -11.680378\n","   -11.321727  ]\n","  [ -4.551235     1.319999   -13.368025   ... -17.926964   -22.727774\n","   -13.414383  ]\n","  [ -4.588898     1.8007125  -14.297414   ... -20.421148   -18.789423\n","   -14.30261   ]\n","  ...\n","  [  9.643804    -5.714846   -19.81092    ... -38.63371    -37.63495\n","   -20.086573  ]\n","  [ 10.260784    -6.555721   -20.531427   ... -40.036415   -39.188263\n","   -20.771585  ]\n","  [ 10.362874    -6.331723   -20.37982    ... -40.427944   -39.06434\n","   -20.650194  ]]], shape=(256, 14, 12001), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NDye-nWsHWJL"},"source":["- output shape에서 256은 배치 사이즈, 14는 시퀀스의 길이(max_len) 그리고 12001은 단어의 개수입니다. 모델의 output은 각 배치에서 각 시퀀스 위치에 들어갈 확률이 높은 단어를 통계적으로 구해줍니다. 즉 12001개의 단어장 길이만큼 계산을 하고, 이것이 시퀀스 길이에 해당하는 만큼 있으며 배치 사이즈 전체에 나타납니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zbezu5lBvQfm","executionInfo":{"status":"ok","timestamp":1627350684343,"user_tz":-540,"elapsed":1134473,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"ccece11f-ab11-40ba-8100-40df6ed859a3"},"source":["# 모델을 학습시킵니다.\n","optimizer = tf.keras.optimizers.Adam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","lyricist.compile(loss=loss, optimizer=optimizer)\n","lyricist.fit(dataset, epochs=10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","488/488 [==============================] - 91s 181ms/step - loss: 3.4822\n","Epoch 2/10\n","488/488 [==============================] - 94s 193ms/step - loss: 3.0254\n","Epoch 3/10\n","488/488 [==============================] - 95s 194ms/step - loss: 2.8557\n","Epoch 4/10\n","488/488 [==============================] - 94s 192ms/step - loss: 2.7255\n","Epoch 5/10\n","488/488 [==============================] - 94s 192ms/step - loss: 2.6180\n","Epoch 6/10\n","488/488 [==============================] - 94s 192ms/step - loss: 2.5214\n","Epoch 7/10\n","488/488 [==============================] - 94s 192ms/step - loss: 2.4315\n","Epoch 8/10\n","488/488 [==============================] - 94s 193ms/step - loss: 2.3469\n","Epoch 9/10\n","488/488 [==============================] - 94s 193ms/step - loss: 2.2673\n","Epoch 10/10\n","488/488 [==============================] - 94s 192ms/step - loss: 2.1909\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f142faf1290>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rrjY4DpsFv7F","executionInfo":{"status":"ok","timestamp":1627350684344,"user_tz":-540,"elapsed":38,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"ab937480-ff58-4ffd-fbfa-aae40cf8ccc9"},"source":["lyricist.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"text_generator\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        multiple                  3072256   \n","_________________________________________________________________\n","lstm (LSTM)                  multiple                  5246976   \n","_________________________________________________________________\n","lstm_1 (LSTM)                multiple                  8392704   \n","_________________________________________________________________\n","dense (Dense)                multiple                  12301025  \n","=================================================================\n","Total params: 29,012,961\n","Trainable params: 29,012,961\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kxTP9NujFzj_"},"source":["- 작사가 모델 lyricist는 \"text_generator\" 이며 29,012,961개 파라미터를 가지고 있습니다."]},{"cell_type":"markdown","metadata":{"id":"kBKwyVZJCp_x"},"source":["### 3-2) lyricist 모델 평가하기"]},{"cell_type":"markdown","metadata":{"id":"7n_NKodhysAX"},"source":["#### 3-2-1) 작문을 해주는 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"g0BpHCuSvQhB"},"source":["# 학습이 된 모델에게 작문을 시켜봅니다.\n","# 시작문장을 전달하면 작문을 해줍니다. \n","def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n","    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","    end_token = tokenizer.word_index[\"<end>\"]\n","\n","    # 단어 하나씩 예측해 문장을 만듭니다\n","    #    1. 입력받은 문장의 텐서를 입력합니다\n","    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n","    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n","    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n","    while True:\n","        # 1\n","        predict = model(test_tensor) \n","        # 2\n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n","        # 3 \n","        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n","        # 4\n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = \"\"\n","    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","\n","    return generated"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6wdzF-D0wSpi"},"source":["아래 그림은 tensorflow 홈페이지에서 가져온 것으로, text generator 모델이 문장을 만드는 과정을 보여줍니다.     \n","\n","<img src=\"https://drive.google.com/uc?id=1d--UTbmp3h0Ks5cnV2v4TJAfwYOk3oTi\" width=\"500\" />   \n","출처: https://www.tensorflow.org/text/tutorials/text_generation\n"]},{"cell_type":"markdown","metadata":{"id":"MXZswmgfCOJ9"},"source":["#### 3-2-2) Lyricist 모델 평가: 다양한 시작문장을 주고 가사를 작문해보기\n","- 꽤 그럴듯한 가사가 나왔네요! 중간중간 나쁜말도 있는데 학습한 노래 가사에 많이 등장했나보군요."]},{"cell_type":"code","metadata":{"id":"6U474ktAvQlL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627352090876,"user_tz":-540,"elapsed":654,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"c72fb985-0dd8-4638-98d7-debbb08ab313"},"source":["# 10줄 노래 만들기\n","l_1 = generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\")\n","l_2 = generate_text(lyricist, tokenizer, init_sentence=\"<start> i miss\")\n","l_3 = generate_text(lyricist, tokenizer, init_sentence=\"<start> i think\")\n","l_4 = generate_text(lyricist, tokenizer, init_sentence=\"<start> you are\")\n","l_5 = generate_text(lyricist, tokenizer, init_sentence=\"<start> it is\")\n","l_6 = generate_text(lyricist, tokenizer, init_sentence=\"<start> oh\")\n","l_7 = generate_text(lyricist, tokenizer, init_sentence=\"<start> yo\")\n","l_8 = generate_text(lyricist, tokenizer, init_sentence=\"<start> i m\")\n","l_9 = generate_text(lyricist, tokenizer, init_sentence=\"<start> what\")\n","l_10 = generate_text(lyricist, tokenizer, init_sentence=\"<start> but\")\n","print(\"title: AI written song\", l_1, l_2, l_3, l_4, l_5, l_6, l_7, l_8, l_9, l_10, sep=\"\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["title: AI written song\n","<start> i love you <end> \n","<start> i miss you <end> \n","<start> i think i m a motherfucking monster <end> \n","<start> you are the one that you re with me <end> \n","<start> it is a beautiful day <end> \n","<start> oh , oh , oh , oh , oh , oh , oh <end> \n","<start> yo , i m the one , i m a bad bitch <end> \n","<start> i m the one gon hold you down <end> \n","<start> what you want nixga what you what you want nixga <end> \n","<start> but i m not gonna be the same <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H__PcD7k5mrk","executionInfo":{"status":"ok","timestamp":1627356066424,"user_tz":-540,"elapsed":918,"user":{"displayName":"정민지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-T8uaLCtJfZv06G-ohKl0S6E9BSRHeMZ4OkEh6Q=s64","userId":"11891656583637878010"}},"outputId":"fab7de8d-025c-4342-b4ec-d6c0991297e5"},"source":["# 테스트 데이터의 source 문장 10개를 input으로 주고 target 문장과 비교해봅니다.\n","# target 문장 처음 10개를 리스트에 저장합니다.\n","list_target=[]\n","for idx, sentence in enumerate(dec_val):\n","  if idx >= 10: continue\n","  target_sentence=\"\"\n","  for word_index in sentence:\n","    if word_index == 0 : continue    \n","    target_sentence += tokenizer.index_word[word_index] + \" \"\n","  list_target.append(target_sentence)\n","\n","for idx, sentence in enumerate(enc_val):\n","  if idx >= 10: continue\n","  init_sentence = \"\"\n","\n","  for i, word_index in enumerate(sentence):\n","    if i >=2 : continue\n","    init_sentence += tokenizer.index_word[word_index] + \" \"\n","  print(\"작사가 모델의 가사:\", generate_text(lyricist, tokenizer, init_sentence))\n","  print(\"target 문장:\", list_target[idx], \"\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["작사가 모델의 가사: <start> everybody knows that i m a motherfucking monster <end> \n","target 문장: everybody in the club goin be rockin when i m through <end>  \n","\n","작사가 모델의 가사: <start> nigga , i m a voodoo chile <end> \n","target 문장: nigga how sick <end>  \n","\n","작사가 모델의 가사: <start> while i m the only one that s the same <end> \n","target 문장: while i m waiting , while i m waiting for my turn <end>  \n","\n","작사가 모델의 가사: <start> and i m not throwing away my shot <end> \n","target 문장: and left us all alone <end>  \n","\n","작사가 모델의 가사: <start> crazy , i m a voodoo <end> \n","target 문장: crazy bout the way you can be , yes i am <end>  \n","\n","작사가 모델의 가사: <start> and i m not throwing away my shot <end> \n","target 문장: and don t you pretend we didn t <end>  \n","\n","작사가 모델의 가사: <start> mi amor eres tu <end> \n","target 문장: mi wi give you everything weh deh in my wallet <end>  \n","\n","작사가 모델의 가사: <start> stone free , i m not leaving you <end> \n","target 문장: stone free to ride on the breeze <end>  \n","\n","작사가 모델의 가사: <start> you re the only one who knows <end> \n","target 문장: you need to check my swag and get up to date <end>  \n","\n","작사가 모델의 가사: <start> in the <unk> of the <unk> <end> \n","target 문장: in an interstellar burst <end>  \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WtXSHsj4haLJ"},"source":["## 4. 회고\n","**\\<lyricist 모델 평가에 대한 회고\\>**\n","- 작사가 모델은 테스트 데이터셋과 분리된 훈련 데이터셋을 바탕으로 작문을 해주기 때문에 작사한 것이 테스트 데이터셋의 target 문장과는 다릅니다. 물론 타겟문장이 좀 더 자연스럽긴 하지만, 작사가 모델도 어느 정도 말이 되는 가사를 생성해줬습니다.\n","- 하지만 타겟문장과 비교해보면 작사가 모델의 창의성은 다소 떨어지는 것 같습니다. 훈련받은 데이터에서 높은 확률로 등장한 단어들을 생성하기 때문에 자주 등장하는 단어들이 있는 것 같습니다.\n","- 또한 작사가 모델은 단어장에 포함되지 않은 단어들은 \\<unk\\> 로 출력하고 있어서, 더 많은 언어 다양성을 담고 있지 못합니다.\n","\n","<br>\n","\n","**\\<총평\\>**\n","- 작사가 모델이 문장을 생성할 수 있는 것은 많은 문장 데이터(시퀀스)를 학습한 결과이며, 입력받은 단어 다음에 올 단어를 계속 순환적으로 구해주는 알고리즘의 덕분입니다.(RNN)\n","- 자연어 처리 중 텍스트를 다룰 때 중요한 것은 텍스트를 전처리하는 것입니다. 실습에서도 텍스트 데이터를 가져와서 먼저 정규표현식을 활용해서 원하는 형태로 텍스트를 가공해주었습니다. 그리고 이후에 텐서플로우의 tf.keras.preprocessing.text.Tokenizer 를 활용해 토큰화와 텐서로 변형을 진행해주었습니다. 토큰화라는 것이 재미있는 개념인데, 예를 들면 텍스트 데이터를 공백을 기준으로 나눠서 각 단어나 문장부호 등을 숫자에 대응시켜 사전처럼 사용하는 것입니다. 기계는 사람의 언어를 이해하지 못하므로 기계의 언어로 바꿔주는 과정입니다. 모델학습 단계에서는 사람이 알기 쉬운 문장이 아닌, 텐서를 가지고 학습을 하게 됩니다.\n","- 작사가 모델이 만들어준 노랫말을 확인해보니 그럴 듯한 노래 구절이어서 놀라웠습니다.\n","- 자연어 처리를 해보면서 흥미로웠고, 관련해서 더 공부해보고 싶다는 생각이 들었습니다. 또한 작사가 모델은 학습데이터로 외국가수들의 노래가사를 훈련했는데, 우리나라 노래 가사를 훈련하면 어떤 작사를 해줄지 궁금했습니다."]}]}