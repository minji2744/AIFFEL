{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southeast-fashion",
   "metadata": {},
   "source": [
    "#  E15. 트랜스포머로 챗봇 만들기\n",
    "- [songys github](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)에서 데이터셋을 가져와서 한글 질문에 대한 답변을 트랜스포머 모델에 학습한 후 챗봇을 만들어본다.\n",
    "- AI 허브의 정신건강 상담 데이터셋을 추가로 활용. [AI 허브 데이터셋](https://aihub.or.kr/keti_data_board/language_intelligence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-african",
   "metadata": {},
   "source": [
    "**목차**\n",
    "1. 데이터 로드 및 전처리\n",
    "3. SubwordTextEncoder\n",
    "4. 모델 구성하기\n",
    "5. 모델 평가하기\n",
    "6. KoBERT로 챗봇 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ancient-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 import\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "increasing-freeware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.7-py2.py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.7\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-telephone",
   "metadata": {},
   "source": [
    "### 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "educated-brand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SD카드 망가졌어</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SD카드 안돼</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SNS 맞팔 왜 안하지ㅠㅠ</td>\n",
       "      <td>잘 모르고 있을 수도 있어요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SNS 시간낭비인 거 아는데 매일 하는 중</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SNS 시간낭비인데 자꾸 보게됨</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SNS보면 나만 빼고 다 행복해보여</td>\n",
       "      <td>자랑하는 자리니까요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>가끔 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>가끔 뭐하는지 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>가끔은 혼자인게 좋다</td>\n",
       "      <td>혼자를 즐기세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>가난한 자의 설움</td>\n",
       "      <td>돈은 다시 들어올 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>가만 있어도 땀난다</td>\n",
       "      <td>땀을 식혀주세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>가상화폐 쫄딱 망함</td>\n",
       "      <td>어서 잊고 새출발 하세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>가스불 켜고 나갔어</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>가스불 켜놓고 나온거 같아</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>가스비 너무 많이 나왔다.</td>\n",
       "      <td>다음 달에는 더 절약해봐요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Q                    A  label\n",
       "0                    12시 땡!           하루가 또 가네요.      0\n",
       "1               1지망 학교 떨어졌어            위로해 드립니다.      0\n",
       "2              3박4일 놀러가고 싶다          여행은 언제나 좋죠.      0\n",
       "3           3박4일 정도 놀러가고 싶다          여행은 언제나 좋죠.      0\n",
       "4                   PPL 심하네           눈살이 찌푸려지죠.      0\n",
       "5                 SD카드 망가졌어   다시 새로 사는 게 마음 편해요.      0\n",
       "6                   SD카드 안돼   다시 새로 사는 게 마음 편해요.      0\n",
       "7            SNS 맞팔 왜 안하지ㅠㅠ     잘 모르고 있을 수도 있어요.      0\n",
       "8   SNS 시간낭비인 거 아는데 매일 하는 중        시간을 정하고 해보세요.      0\n",
       "9         SNS 시간낭비인데 자꾸 보게됨        시간을 정하고 해보세요.      0\n",
       "10      SNS보면 나만 빼고 다 행복해보여          자랑하는 자리니까요.      0\n",
       "11                   가끔 궁금해        그 사람도 그럴 거예요.      0\n",
       "12              가끔 뭐하는지 궁금해        그 사람도 그럴 거예요.      0\n",
       "13              가끔은 혼자인게 좋다            혼자를 즐기세요.      0\n",
       "14                가난한 자의 설움       돈은 다시 들어올 거예요.      0\n",
       "15               가만 있어도 땀난다            땀을 식혀주세요.      0\n",
       "16               가상화폐 쫄딱 망함       어서 잊고 새출발 하세요.      0\n",
       "17               가스불 켜고 나갔어  빨리 집에 돌아가서 끄고 나오세요.      0\n",
       "18           가스불 켜놓고 나온거 같아  빨리 집에 돌아가서 끄고 나오세요.      0\n",
       "19           가스비 너무 많이 나왔다.      다음 달에는 더 절약해봐요.      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_data = os.getenv('HOME') + '/aiffel/transformer_chatbot/data/ChatbotData .csv'\n",
    "\n",
    "chat = pd.read_csv(path_to_data)\n",
    "chat.head(20)  # 20개 샘플을 확인하여 전처리에 대한 아이디어 얻기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-spell",
   "metadata": {},
   "source": [
    "- ChatbotData.csv 파일을 읽어보니, chat['Q']에는 질문이, chat['A']에는 답변이 쌍을 이룬 데이터이다. 이를 활용해서 병렬데이터를 구축하고 학습을 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "second-conflict",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11823, 3)\n"
     ]
    }
   ],
   "source": [
    "print(chat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dutch-gender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 고유 라벨 확인\n",
    "chat['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incorporate-tiffany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5290</th>\n",
       "      <td>1000일 만난 여자친구와 이별</td>\n",
       "      <td>더 오래 만날 사람 만날 거예요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5291</th>\n",
       "      <td>10년 연애. 헤어졌습니다.</td>\n",
       "      <td>더 공허함이 크시겠네요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5292</th>\n",
       "      <td>10년 연애사 되돌아보니 다 부질없네</td>\n",
       "      <td>더 좋은 사람 만나실 거예요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5293</th>\n",
       "      <td>10년 연예의끝</td>\n",
       "      <td>더 마음이 허하겠어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5294</th>\n",
       "      <td>10년만나다 헤어지네</td>\n",
       "      <td>충분히 슬퍼하고 충분히 아파하다가 이겨내세요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8855</th>\n",
       "      <td>힘듭니다.</td>\n",
       "      <td>어떤 말도 위로가 되지 않겠지만 힘내세요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8856</th>\n",
       "      <td>힘이 될런지 모르겠지만</td>\n",
       "      <td>어떤 말도 위로가 되지 않겠지만 힘내세요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8857</th>\n",
       "      <td>힘이 드네</td>\n",
       "      <td>어떤 말도 위로가 되지 않겠지만 힘내세요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8858</th>\n",
       "      <td>힘이 없어</td>\n",
       "      <td>힘내세요!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8859</th>\n",
       "      <td>힘이드네.여자들이란.</td>\n",
       "      <td>어떤 말도 위로가 되지 않겠지만 힘내세요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3570 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Q                          A  label\n",
       "5290     1000일 만난 여자친구와 이별         더 오래 만날 사람 만날 거예요.      1\n",
       "5291       10년 연애. 헤어졌습니다.              더 공허함이 크시겠네요.      1\n",
       "5292  10년 연애사 되돌아보니 다 부질없네           더 좋은 사람 만나실 거예요.      1\n",
       "5293              10년 연예의끝               더 마음이 허하겠어요.      1\n",
       "5294           10년만나다 헤어지네  충분히 슬퍼하고 충분히 아파하다가 이겨내세요.      1\n",
       "...                    ...                        ...    ...\n",
       "8855                 힘듭니다.    어떤 말도 위로가 되지 않겠지만 힘내세요.      1\n",
       "8856          힘이 될런지 모르겠지만    어떤 말도 위로가 되지 않겠지만 힘내세요.      1\n",
       "8857                 힘이 드네    어떤 말도 위로가 되지 않겠지만 힘내세요.      1\n",
       "8858                 힘이 없어                      힘내세요!      1\n",
       "8859           힘이드네.여자들이란.    어떤 말도 위로가 되지 않겠지만 힘내세요.      1\n",
       "\n",
       "[3570 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat[chat['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "illegal-animal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8860</th>\n",
       "      <td>짝사랑만큼 고통스러운 건 없겠지.</td>\n",
       "      <td>짝사랑 만큼 감정소모가 큰 건 없을 거예요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8861</th>\n",
       "      <td>1년 넘게 만났는데 지금도 불타올라</td>\n",
       "      <td>정열적인 사랑을 하고 있나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8862</th>\n",
       "      <td>1년 동거 중인데 계속 좋아</td>\n",
       "      <td>서로 깊게 알게되면서 더 좋아졌나봅니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8863</th>\n",
       "      <td>1년 동거하고 결혼했어</td>\n",
       "      <td>축하합니다!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8864</th>\n",
       "      <td>1년 만났는데도 그 사람에 대해 잘 모르겠어</td>\n",
       "      <td>더 만나보세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2963 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Q                         A  label\n",
       "8860         짝사랑만큼 고통스러운 건 없겠지.  짝사랑 만큼 감정소모가 큰 건 없을 거예요.      2\n",
       "8861        1년 넘게 만났는데 지금도 불타올라         정열적인 사랑을 하고 있나봐요.      2\n",
       "8862            1년 동거 중인데 계속 좋아    서로 깊게 알게되면서 더 좋아졌나봅니다.      2\n",
       "8863               1년 동거하고 결혼했어                    축하합니다!      2\n",
       "8864   1년 만났는데도 그 사람에 대해 잘 모르겠어                  더 만나보세요.      2\n",
       "...                         ...                       ...    ...\n",
       "11818            훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819            훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820               흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821   힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822                힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[2963 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat[chat['label'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-affect",
   "metadata": {},
   "source": [
    "- 데이터 제작자는 라벨에 대해 이렇게 설명하고 있습니다 :  일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sticky-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI 허브에서 정신건강 대화 데이터셋을 가져오기\n",
    "data_path = os.getenv('HOME') + '/aiffel/transformer_chatbot/data/wellness_1.xlsx'\n",
    "wellness = pd.read_excel(data_path, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "great-bacteria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>구분</th>\n",
       "      <th>유저</th>\n",
       "      <th>챗봇</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.</td>\n",
       "      <td>감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>더 이상 내 감정을 내가 컨트롤 못 하겠어.</td>\n",
       "      <td>저도 그 기분 이해해요. 많이 힘드시죠?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>하루종일 오르락내리락 롤러코스터 타는 기분이에요.</td>\n",
       "      <td>그럴 때는 밥은 잘 먹었는지, 잠은 잘 잤는지 체크해보는 것도 좋아요.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>꼭 롤러코스터 타는 것 같아요.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>롤러코스터 타는 것처럼 기분이 왔다 갔다 해요.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>작년 가을부터 감정조절이 잘 안 되는 거 같아.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>예전에 비해서 인내심이 너무 짧아진 거 같아.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>더 이상 혼자서는 감정조절을 못하겠어.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>점점 나 자신을 컨트롤하지 못하는 기분이야.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>나도 이러기 싫은데 내 마음대로 안돼.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>맨정신일 때는 저를 주체할 수 가 없었거든요.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>주체가 안 돼.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>이렇게 쌓이고 쌓이다 나중에 확 터지거든요. 진짜 걷잡을 수 없이요.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>근데 감정을 다스리지 못해 욱하기도하고.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>순간순간 감정조절을 못해요.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>감정/감정조절이상/화</td>\n",
       "      <td>평소 다른 일을 할 때도 비슷해요. 생각한대로 안되면 화가 나고…그런 상황이 지속되...</td>\n",
       "      <td>화가 폭발할 것 같을 때는 그 자리를 피하는 것도 좋은 방법이라고 생각해요.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>감정/감정조절이상/화</td>\n",
       "      <td>예전보다 화내는 게 과격해진 거 같아.</td>\n",
       "      <td>정말 힘드시겠어요. 화는 남에게도 스스로에게도 상처를 주잖아요.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>감정/감정조절이상/화</td>\n",
       "      <td>화가 안 참아져.</td>\n",
       "      <td>화가 너무 많이 날 때는 심호흡을 해보는 게 어떨까요? 씁- 후-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>감정/감정조절이상/화</td>\n",
       "      <td>근데 다음에 또 그러면 또 화가 나고… 모르겠어요. 제 감정이 통제가 안 돼요.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>감정/감정조절이상/화</td>\n",
       "      <td>마음은 안 그런데 화를 낼 때 더 불같이 내게 되기도 하고..</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             구분                                                 유저  \\\n",
       "0     감정/감정조절이상    제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.   \n",
       "1     감정/감정조절이상                           더 이상 내 감정을 내가 컨트롤 못 하겠어.   \n",
       "2     감정/감정조절이상                        하루종일 오르락내리락 롤러코스터 타는 기분이에요.   \n",
       "3     감정/감정조절이상                                  꼭 롤러코스터 타는 것 같아요.   \n",
       "4     감정/감정조절이상                         롤러코스터 타는 것처럼 기분이 왔다 갔다 해요.   \n",
       "5     감정/감정조절이상                         작년 가을부터 감정조절이 잘 안 되는 거 같아.   \n",
       "6     감정/감정조절이상                          예전에 비해서 인내심이 너무 짧아진 거 같아.   \n",
       "7     감정/감정조절이상                              더 이상 혼자서는 감정조절을 못하겠어.   \n",
       "8     감정/감정조절이상                           점점 나 자신을 컨트롤하지 못하는 기분이야.   \n",
       "9     감정/감정조절이상                              나도 이러기 싫은데 내 마음대로 안돼.   \n",
       "10    감정/감정조절이상                          맨정신일 때는 저를 주체할 수 가 없었거든요.   \n",
       "11    감정/감정조절이상                                           주체가 안 돼.   \n",
       "12    감정/감정조절이상             이렇게 쌓이고 쌓이다 나중에 확 터지거든요. 진짜 걷잡을 수 없이요.   \n",
       "13    감정/감정조절이상                             근데 감정을 다스리지 못해 욱하기도하고.   \n",
       "14    감정/감정조절이상                                    순간순간 감정조절을 못해요.   \n",
       "15  감정/감정조절이상/화  평소 다른 일을 할 때도 비슷해요. 생각한대로 안되면 화가 나고…그런 상황이 지속되...   \n",
       "16  감정/감정조절이상/화                              예전보다 화내는 게 과격해진 거 같아.   \n",
       "17  감정/감정조절이상/화                                          화가 안 참아져.   \n",
       "18  감정/감정조절이상/화       근데 다음에 또 그러면 또 화가 나고… 모르겠어요. 제 감정이 통제가 안 돼요.   \n",
       "19  감정/감정조절이상/화                 마음은 안 그런데 화를 낼 때 더 불같이 내게 되기도 하고..   \n",
       "\n",
       "                                            챗봇  Unnamed: 3  \n",
       "0              감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.         NaN  \n",
       "1                       저도 그 기분 이해해요. 많이 힘드시죠?         NaN  \n",
       "2      그럴 때는 밥은 잘 먹었는지, 잠은 잘 잤는지 체크해보는 것도 좋아요.         NaN  \n",
       "3                                          NaN         NaN  \n",
       "4                                          NaN         NaN  \n",
       "5                                          NaN         NaN  \n",
       "6                                          NaN         NaN  \n",
       "7                                          NaN         NaN  \n",
       "8                                          NaN         NaN  \n",
       "9                                          NaN         NaN  \n",
       "10                                         NaN         NaN  \n",
       "11                                         NaN         NaN  \n",
       "12                                         NaN         NaN  \n",
       "13                                         NaN         NaN  \n",
       "14                                         NaN         NaN  \n",
       "15  화가 폭발할 것 같을 때는 그 자리를 피하는 것도 좋은 방법이라고 생각해요.         NaN  \n",
       "16         정말 힘드시겠어요. 화는 남에게도 스스로에게도 상처를 주잖아요.         NaN  \n",
       "17        화가 너무 많이 날 때는 심호흡을 해보는 게 어떨까요? 씁- 후-         NaN  \n",
       "18                                         NaN         NaN  \n",
       "19                                         NaN         NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wellness.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-mailman",
   "metadata": {},
   "source": [
    "- wellness 데이터셋을 chat 데이터셋과 concatenate 해서 조금 더 풍부하게 데이터셋을 구성해보고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "authentic-salvation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "구분               0\n",
       "유저               0\n",
       "챗봇            4197\n",
       "Unnamed: 3    5231\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인\n",
    "wellness.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fluid-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5231, 4)\n"
     ]
    }
   ],
   "source": [
    "print(wellness.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "funded-cycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1034, 3)\n"
     ]
    }
   ],
   "source": [
    "wellness = wellness.drop(['Unnamed: 3'], axis=1)\n",
    "wellness.dropna(inplace=True)\n",
    "print(wellness.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acting-beverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['감정/감정조절이상', '감정/감정조절이상/화', '감정/걱정', '감정/걱정/건강문제', '감정/걱정/건강염려',\n",
       "       '감정/걱정/경제적문제', '감정/걱정/미래', '감정/걱정/불면', '감정/걱정/암', '감정/걱정/자녀',\n",
       "       '감정/걱정/주변평가', '감정/걱정/증상재발', '감정/고독감', '감정/곤혹감', '감정/공포', '감정/공포/새',\n",
       "       '감정/공허감', '감정/과민반응', '감정/괴로움', '감정/기분저하', '감정/기시감', '감정/긴장',\n",
       "       '감정/눈물', '감정/답답', '감정/답답/사람많은곳', '감정/당황', '감정/두려움', '감정/두려움/운전',\n",
       "       '감정/두려움/자동차', '감정/멍함', '감정/모호함', '감정/무력감', '감정/무미건조', '감정/무서움',\n",
       "       '감정/미안함/자녀', '감정/미움', '감정/배신감', '감정/부정적사고', '감정/분노', '감정/불만',\n",
       "       '감정/불신', '감정/불안감', '감정/불안감/긴장', '감정/불안감/미래', '감정/불안감/증상재발',\n",
       "       '감정/불안감/초조함', '감정/불쾌감', '감정/불편감', '감정/비관적', '감정/살인욕구', '감정/생각',\n",
       "       '감정/서운함', '감정/속상함', '감정/슬픔', '감정/신경쓰임', '감정/심란', '감정/억울함',\n",
       "       '감정/예민함', '감정/외로움', '감정/우울감', '감정/우울감/눈물', '감정/우울감/증상재발',\n",
       "       '감정/우울감/증상지속', '감정/의기소침', '감정/의기소침/자격지심', '감정/의욕상실', '감정/의욕상실/무기력',\n",
       "       '감정/자괴감', '감정/자살충동', '감정/자신감저하', '감정/자존감저하', '감정/절망감', '감정/좌절',\n",
       "       '감정/죄책감', '감정/즐거움', '감정/짜증', '감정/창피함', '감정/초조함', '감정/충격',\n",
       "       '감정/통제력상실', '감정/허무함', '감정/화', '감정/후회', '감정/후회/결혼', '감정/힘듦',\n",
       "       '감정/힘듦/스트레스', '감정/힘듦/지침', '내원이유/상담', '내원이유/의사소견', '내원이유/치료', '모호함',\n",
       "       '배경/가족', '배경/가족/갈등', '배경/가족/무관심', '배경/건강문제', '배경/건강문제/갑상선',\n",
       "       '배경/건강문제/다이어트', '배경/건강문제/다이어트/스트레스', '배경/건강문제/생리불순', '배경/건강문제/수술',\n",
       "       '배경/건강문제/알레르기', '배경/건강문제/항암', '배경/결혼', '배경/결혼/미혼', '배경/경제적문제',\n",
       "       '배경/경제적문제/가난', '배경/경제적문제/빚', '배경/공부', '배경/공부/부진', '배경/군대/군입대',\n",
       "       '배경/귀국', '배경/남자친구', '배경/남자친구/고민/없음', '배경/남자친구/동경', '배경/남자친구/없음',\n",
       "       '배경/남자친구/이별', '배경/남자친구/집착', '배경/남자친구/짧은교제', '배경/남편', '배경/남편/갈등',\n",
       "       '배경/남편/경제적문제', '배경/남편/과음', '배경/남편/관계소원', '배경/남편/관계양호', '배경/남편/다툼',\n",
       "       '배경/남편/무관심', '배경/남편/바람', '배경/남편/사업', '배경/남편/소통불가', '배경/남편/의심',\n",
       "       '배경/남편/의지', '배경/남편/폭력', '배경/대인관계', '배경/대인관계/갈등', '배경/대인관계/양호',\n",
       "       '배경/대인관계/협소', '배경/대학', '배경/대학/실패', '배경/대학/실패/재수', '배경/대학/입학',\n",
       "       '배경/대학/재수', '배경/대학/휴학', '배경/문제', '배경/문제/과음', '배경/문제/머리카락/털뽑기',\n",
       "       '배경/문제/불면', '배경/문제/불안감/소변', '배경/문제/불편감/옷', '배경/문제/소변',\n",
       "       '배경/문제/알코올의존', '배경/부모', '배경/부모/가출/아버지', '배경/부모/갈등', '배경/부모/갈등/아버지',\n",
       "       '배경/부모/관계소원', '배경/부모/무관심', '배경/부모/싸움', '배경/부모/아버지', '배경/부모/아버지/죽음',\n",
       "       '배경/부모/아버지/폭력', '배경/부모/어머니/죽음', '배경/부모/이혼', '배경/사고', '배경/사고/교통사고',\n",
       "       '배경/사업', '배경/사업/경제적문제/실패', '배경/사업/남편/동업자', '배경/사업/실패', '배경/생활',\n",
       "       '배경/생활/불가능/운전', '배경/생활/스트레스', '배경/생활/양호', '배경/생활/여행/해외',\n",
       "       '배경/생활/운동', '배경/생활/자연소멸/증상', '배경/생활/취미', '배경/생활/폭행/피해', '배경/생활/해외',\n",
       "       '배경/생활/혼자', '배경/생활/휴식', '배경/성격', '배경/성격/극단적', '배경/성격/급함',\n",
       "       '배경/성격/내성적', '배경/성격/소심', '배경/성격/예민함', '배경/성격/완벽추구', '배경/성격/욕심많음',\n",
       "       '배경/성격/자기중심적', '배경/성격/자립적', '배경/시댁', '배경/시댁/갈등', '배경/시댁/갈등/시어머니',\n",
       "       '배경/시댁/경제적문제', '배경/아르바이트', '배경/애완동물', '배경/애완동물/가족/갈등', '배경/어린시절',\n",
       "       '배경/어린시절/가난', '배경/여자친구', '배경/여자친구/관계소원', '배경/여자친구/동거',\n",
       "       '배경/여자친구/이별', '배경/연애', '배경/연애/이별', '배경/유학', '배경/육아/힘듦', '배경/음주',\n",
       "       '배경/음주/과음', '배경/음주/알코올의존', '배경/음주/애주가', '배경/음주/자주', '배경/이사',\n",
       "       '배경/이혼', '배경/임신', '배경/임신/낙태', '배경/자각/우울증', '배경/자각/정신질환', '배경/자녀',\n",
       "       '배경/전연인', '배경/종교', '배경/직장', '배경/직장/고민/퇴사', '배경/직장/과도한업무',\n",
       "       '배경/직장/반복/이직', '배경/직장/복직', '배경/직장/불만', '배경/직장/불만/업무', '배경/직장/스트레스',\n",
       "       '배경/직장/양호', '배경/직장/없음/흥미', '배경/직장/이직', '배경/직장/퇴사', '배경/직장/휴직',\n",
       "       '배경/진로', '배경/취업', '배경/취업/준비', '배경/취업/힘듦', '배경/친구', '배경/친구/관계소원',\n",
       "       '배경/친구/배신', '배경/친구/없음', '배경/타인/갈등', '배경/학교', '배경/학교/갈등/선생님',\n",
       "       '배경/학교/결석', '배경/학교/따돌림', '배경/학교/자퇴', '배경/학업', '배경/학업/부진',\n",
       "       '배경/학업/양호', '배경/학업/우수', '부가설명', '상태/양호', '상태/증상감소', '상태/증상지속',\n",
       "       '원인/없음', '자가치료/심리조절', '자가치료/심리조절/증상지속', '자가치료/운동', '자가치료/충분한휴식',\n",
       "       '증상/가슴답답', '증상/가슴떨림', '증상/가슴통증', '증상/건강염려', '증상/공격적성향', '증상/공황발작',\n",
       "       '증상/과대망상', '증상/과수면', '증상/기억력저하', '증상/기억력저하/집중력저하', '증상/기억상실',\n",
       "       '증상/기절', '증상/기절예기', '증상/대인기피', '증상/대화기피', '증상/두근거림', '증상/두근거림/불면',\n",
       "       '증상/두통', '증상/두통/불면', '증상/떨림', '증상/만성피로', '증상/메스꺼움', '증상/무기력',\n",
       "       '증상/무기력/은둔', '증상/무기력/의욕상실', '증상/반복사고', '증상/반복사고/트라우마', '증상/반복행동',\n",
       "       '증상/반복행동/대화', '증상/반복행동/문단속', '증상/반복행동/손씻기', '증상/반복행동/확인', '증상/발작',\n",
       "       '증상/불면', '증상/불면/불안감', '증상/불면/생각많음', '증상/불면/스트레스', '증상/불면/예민함',\n",
       "       '증상/불면/증상지속', '증상/불면/피로', '증상/생리불순', '증상/성격변화', '증상/성욕상승',\n",
       "       '증상/소화불량', '증상/속쓰림', '증상/시력저하', '증상/식욕저하', '증상/식욕저하/불면',\n",
       "       '증상/식욕저하/체중감소', '증상/신체이상/목', '증상/악몽', '증상/알코올의존', '증상/어지러움',\n",
       "       '증상/은둔', '증상/이명', '증상/이인감', '증상/인지기능저하', '증상/자살시도', '증상/자해',\n",
       "       '증상/저림현상/발/손', '증상/죽음공포', '증상/죽음공포/호흡곤란', '증상/집중력저하', '증상/체력저하',\n",
       "       '증상/체중감소', '증상/체중증가', '증상/컨디션저조', '증상/통증', '증상/통증/목', '증상/통증/전신',\n",
       "       '증상/통증/허리', '증상/편두통', '증상/폭식', '증상/폭식/스트레스/해소', '증상/피로',\n",
       "       '증상/피로/불면', '증상/피해망상', '증상/피해망상/감시', '증상/피해망상/감시/남편', '증상/피해망상/남편',\n",
       "       '증상/피해망상/도청', '증상/호흡곤란', '증상/호흡곤란/가슴답답', '증상/환각', '증상/환청',\n",
       "       '증상/힘빠짐', '치료이력/검사', '치료이력/검사/이상없음', '치료이력/병원내원',\n",
       "       '치료이력/병원내원/검사/이상없음', '치료이력/병원내원/복약', '치료이력/병원내원/이상없음', '치료이력/응급실',\n",
       "       '현재상태/증상악화'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wellness['구분'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "demographic-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "wellness = wellness[['유저', '챗봇']]\n",
    "wellness = wellness.rename({'유저' : 'Q', '챗봇' : 'A'}, axis = 1)\n",
    "wellness['label'] = np.ones((wellness.shape[0],), dtype=int)  # 부정적인 내용의 데이터이므로 라벨을 1로 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "induced-beverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.</td>\n",
       "      <td>감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>더 이상 내 감정을 내가 컨트롤 못 하겠어.</td>\n",
       "      <td>저도 그 기분 이해해요. 많이 힘드시죠?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>하루종일 오르락내리락 롤러코스터 타는 기분이에요.</td>\n",
       "      <td>그럴 때는 밥은 잘 먹었는지, 잠은 잘 잤는지 체크해보는 것도 좋아요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>평소 다른 일을 할 때도 비슷해요. 생각한대로 안되면 화가 나고…그런 상황이 지속되...</td>\n",
       "      <td>화가 폭발할 것 같을 때는 그 자리를 피하는 것도 좋은 방법이라고 생각해요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>예전보다 화내는 게 과격해진 거 같아.</td>\n",
       "      <td>정말 힘드시겠어요. 화는 남에게도 스스로에게도 상처를 주잖아요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>그 사람이 응급실 의사한테 뭐라고 속닥거리니까, 저보고 갑자기 응급처치 끝났다고, ...</td>\n",
       "      <td>응급실이 있어서 다행이네요. 큰 문제는 없으신 거죠?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>파편이 튀어서 그 때 저도 응급실 가서 치료 받기도 했고…</td>\n",
       "      <td>응급실에 가셨다니 정말 놀랐어요. 아무 문제 없으신가요? 걱정 되네요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>지금 상태가 너무 안 좋아서 학교 안 나가고 있어요.</td>\n",
       "      <td>상태가 더 안 좋아지셨군요. 걱정이 되네요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>진짜 심해진 거 같긴 해요.</td>\n",
       "      <td>정말 힘드시겠어요. 지금도 증상이 심하신가요?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>그런데 증상이 나빠진 거 같아.</td>\n",
       "      <td>너무 심하시면 병원을 다시 가보는 건 어떨까요?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1034 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Q  \\\n",
       "0       제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.   \n",
       "1                              더 이상 내 감정을 내가 컨트롤 못 하겠어.   \n",
       "2                           하루종일 오르락내리락 롤러코스터 타는 기분이에요.   \n",
       "15    평소 다른 일을 할 때도 비슷해요. 생각한대로 안되면 화가 나고…그런 상황이 지속되...   \n",
       "16                                예전보다 화내는 게 과격해진 거 같아.   \n",
       "...                                                 ...   \n",
       "5196  그 사람이 응급실 의사한테 뭐라고 속닥거리니까, 저보고 갑자기 응급처치 끝났다고, ...   \n",
       "5197                   파편이 튀어서 그 때 저도 응급실 가서 치료 받기도 했고…   \n",
       "5213                     지금 상태가 너무 안 좋아서 학교 안 나가고 있어요.    \n",
       "5214                                   진짜 심해진 거 같긴 해요.    \n",
       "5215                                 그런데 증상이 나빠진 거 같아.    \n",
       "\n",
       "                                               A  label  \n",
       "0                감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.      1  \n",
       "1                         저도 그 기분 이해해요. 많이 힘드시죠?      1  \n",
       "2        그럴 때는 밥은 잘 먹었는지, 잠은 잘 잤는지 체크해보는 것도 좋아요.      1  \n",
       "15    화가 폭발할 것 같을 때는 그 자리를 피하는 것도 좋은 방법이라고 생각해요.      1  \n",
       "16           정말 힘드시겠어요. 화는 남에게도 스스로에게도 상처를 주잖아요.      1  \n",
       "...                                          ...    ...  \n",
       "5196               응급실이 있어서 다행이네요. 큰 문제는 없으신 거죠?      1  \n",
       "5197     응급실에 가셨다니 정말 놀랐어요. 아무 문제 없으신가요? 걱정 되네요.      1  \n",
       "5213                    상태가 더 안 좋아지셨군요. 걱정이 되네요.      1  \n",
       "5214                   정말 힘드시겠어요. 지금도 증상이 심하신가요?      1  \n",
       "5215                  너무 심하시면 병원을 다시 가보는 건 어떨까요?      1  \n",
       "\n",
       "[1034 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(wellness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aggressive-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12857, 3)\n"
     ]
    }
   ],
   "source": [
    "chat = pd.concat([chat, wellness])\n",
    "print(chat.shape) # chat과 wellness 합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secondary-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.to_csv('/aiffel/aiffel/aiffel_project/KoGPT2-chatbot/Chatbot_data/ChatbotData.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-yellow",
   "metadata": {},
   "source": [
    "**전처리 프로세스**\n",
    "> 1) 단어와 구두점 분리  \n",
    "> 2) 의미를 가진 단어로 영어, 한글, 숫자를 포함하기  \n",
    "> 3) 구두점, 영어, 한글, 숫자를 제외한 특수문자 등은 공백으로 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "german-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # (영어, 한글, 숫자, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "  sentence = re.sub(r\"[^a-zA-Z|ㄱ-ㅎ|ㅏ-ㅣ|가-힣|?.!,0-9]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "    \n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "nutritional-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
    "def load_conversations(df):\n",
    "  inputs, outputs = [], []\n",
    "\n",
    "  questions = df['Q']\n",
    "  for line in questions:\n",
    "    inputs.append(preprocess_sentence(line))\n",
    "\n",
    "  answers = df['A']\n",
    "  for line in answers:\n",
    "    outputs.append(preprocess_sentence(line))    \n",
    "\n",
    "  return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adjustable-timer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 12857\n",
      "전체 샘플 수 : 12857\n",
      "전처리 후의 22번째 질문 샘플: 가스비 장난 아님\n",
      "전처리 후의 22번째 답변 샘플: 다음 달에는 더 절약해봐요 .\n"
     ]
    }
   ],
   "source": [
    "questions, answers = load_conversations(chat)\n",
    "\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))\n",
    "\n",
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[21]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[21]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-judgment",
   "metadata": {},
   "source": [
    "### 2. SubwordTextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acceptable-roberts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\n",
      "슝=3 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성. (Tensorflow 2.3.0 이상) (클라우드는 2.4 입니다)\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"슝=3 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "plastic-brooklyn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [7658]\n",
      "END_TOKEN의 번호 : [7659]\n",
      "7660\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])\n",
    "\n",
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "useful-costume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5916, 676, 3217, 5317]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2256, 4000, 8, 13, 2858, 736, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fatty-cover",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "specialized-louisville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 7660\n",
      "필터링 후의 질문 샘플 개수: 12857\n",
      "필터링 후의 답변 샘플 개수: 12857\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "hispanic-cigarette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 교사 강요\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-rochester",
   "metadata": {},
   "source": [
    "### 3. 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-camera",
   "metadata": {},
   "source": [
    "[transformer](http://jalammar.github.io/illustrated-transformer/) 모델에 대한 설명은 링크를 참고했고, [\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf) 해당 논문에서 트랜스포머 모델의 구조를 가져왔습니다.\n",
    "\n",
    "![nn](transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "enormous-backup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bronze-campus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hollywood-petroleum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bronze-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "genuine-storage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "private-chassis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "documentary-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "collected-patio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "limiting-greece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "broad-parade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 512)    10233856    inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 512)    14440448    dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 7660)   3929580     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 28,603,884\n",
      "Trainable params: 28,603,884\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 6 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "immune-machinery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "interested-gamma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "governing-safety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cloudy-agenda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "201/201 [==============================] - 82s 335ms/step - loss: 1.5767 - accuracy: 0.0174\n",
      "Epoch 2/20\n",
      "201/201 [==============================] - 68s 339ms/step - loss: 1.2089 - accuracy: 0.0491\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - 69s 341ms/step - loss: 1.0910 - accuracy: 0.0517\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - 69s 345ms/step - loss: 1.0256 - accuracy: 0.0563\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - 69s 345ms/step - loss: 0.9579 - accuracy: 0.0608\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - 69s 344ms/step - loss: 0.8724 - accuracy: 0.0665\n",
      "Epoch 7/20\n",
      "201/201 [==============================] - 69s 345ms/step - loss: 0.7958 - accuracy: 0.0740\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - 70s 346ms/step - loss: 0.7051 - accuracy: 0.0833\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - 70s 346ms/step - loss: 0.6116 - accuracy: 0.0937\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - 69s 344ms/step - loss: 0.5217 - accuracy: 0.1055\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - 69s 344ms/step - loss: 0.4411 - accuracy: 0.1169\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - 69s 345ms/step - loss: 0.3647 - accuracy: 0.1277\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - 69s 346ms/step - loss: 0.2921 - accuracy: 0.1378\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - 69s 341ms/step - loss: 0.2378 - accuracy: 0.1466\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - 70s 346ms/step - loss: 0.1825 - accuracy: 0.1552\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - 69s 344ms/step - loss: 0.1508 - accuracy: 0.1597\n",
      "Epoch 17/20\n",
      "201/201 [==============================] - 69s 345ms/step - loss: 0.1245 - accuracy: 0.1644\n",
      "Epoch 18/20\n",
      "201/201 [==============================] - 69s 344ms/step - loss: 0.1092 - accuracy: 0.1679\n",
      "Epoch 19/20\n",
      "201/201 [==============================] - 70s 347ms/step - loss: 0.0992 - accuracy: 0.1671\n",
      "Epoch 20/20\n",
      "201/201 [==============================] - 69s 345ms/step - loss: 0.0933 - accuracy: 0.1693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff680232fd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-circus",
   "metadata": {},
   "source": [
    "## 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "statistical-boxing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "public-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "victorian-pollution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 저녁 뭐 먹지?\n",
      "출력 : 맛있는 거 드세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'맛있는 거 드세요 .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘 저녁 뭐 먹지?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "tracked-delicious",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 몸이 쑤셔.\n",
      "출력 : 정말 걱정되겠어요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'정말 걱정되겠어요 .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('몸이 쑤셔.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dominant-console",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 졸리다.\n",
      "출력 : 잠을 깨요 ! 기운 내요\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'잠을 깨요 ! 기운 내요'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('졸리다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fixed-laundry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 공부하기 싫어.\n",
      "출력 : 조금만 더 버텨보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'조금만 더 버텨보세요 .'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('공부하기 싫어.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "precise-sellers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘은 날씨가 좋네!\n",
      "출력 : 힘든 결정이었겠어요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'힘든 결정이었겠어요 .'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘은 날씨가 좋네!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fourth-postage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 저녁에 뭐 할까?\n",
      "출력 : 원한다면 그렇게 해도 돼요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'원한다면 그렇게 해도 돼요 .'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘 저녁에 뭐 할까?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "superior-shipping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕! 좋은 아침이야!\n",
      "출력 : 이제 울어도 돼요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이제 울어도 돼요 .'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('안녕! 좋은 아침이야!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "congressional-diversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 나는 정민지야.\n",
      "출력 : 꼬박 챙겨 드세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'꼬박 챙겨 드세요 .'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('나는 정민지야.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-conditions",
   "metadata": {},
   "source": [
    "## 회고\n",
    "- 챗봇이 출력을 해주는 문장을 보면 어느 정도 말도 되고, 재미있는 것 같다.\n",
    "- 챗봇 구현에는 트랜스포머 모델을 사용했으며, 인코더 레이어는 셀프어텐션(멀티헤드어텐션)과 피드포워드로 구성되고, 디코더 레이어는 멀티헤드어텐션, 인코더-디코더 어텐션, 피드포워드로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "green-bahrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet-cu102\n",
      "  Downloading mxnet_cu102-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (377.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 377.2 MB 12 kB/s s eta 0:00:01   |███▎                            | 38.4 MB 6.5 MB/s eta 0:00:53��████▏                 | 166.5 MB 76.2 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /opt/conda/lib/python3.7/site-packages (from mxnet-cu102) (1.19.5)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from mxnet-cu102) (2.25.1)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu102) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu102) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu102) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu102) (2.10)\n",
      "Installing collected packages: graphviz, mxnet-cu102\n",
      "  Attempting uninstall: graphviz\n",
      "    Found existing installation: graphviz 0.16\n",
      "    Uninstalling graphviz-0.16:\n",
      "      Successfully uninstalled graphviz-0.16\n",
      "Successfully installed graphviz-0.8.4 mxnet-cu102-1.8.0.post0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.95)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.9.0+cu111)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.2.1)\n",
      "Collecting gluonnlp\n",
      "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
      "\u001b[K     |████████████████████████████████| 344 kB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (1.19.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (20.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-1.4.5-py3-none-any.whl (919 kB)\n",
      "\u001b[K     |████████████████████████████████| 919 kB 71.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (4.56.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (5.3.1)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (2.4.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (0.18.2)\n",
      "Collecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
      "\u001b[K     |████████████████████████████████| 119 kB 65.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.25.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.7.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.14.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.24.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
      "Collecting torchmetrics>=0.4.0\n",
      "  Downloading torchmetrics-0.5.1-py3-none-any.whl (282 kB)\n",
      "\u001b[K     |████████████████████████████████| 282 kB 67.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (20.3.0)\n",
      "Collecting cython\n",
      "  Using cached Cython-0.29.24-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.4.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.0)\n",
      "Building wheels for collected packages: gluonnlp\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595822 sha256=62461285cde6176198c75e0d6bcc8167b894211ec7e73054c0ee4fbcbf468bbc\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
      "Successfully built gluonnlp\n",
      "Installing collected packages: fsspec, torchmetrics, pyDeprecate, cython, pytorch-lightning, gluonnlp\n",
      "Successfully installed cython-0.29.24 fsspec-2021.8.1 gluonnlp-0.10.0 pyDeprecate-0.3.1 pytorch-lightning-1.4.5 torchmetrics-0.5.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Cloning into 'KoGPT2'...\n",
      "remote: Enumerating objects: 196, done.\u001b[K\n",
      "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
      "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
      "remote: Total 196 (delta 39), reused 40 (delta 15), pack-reused 121\u001b[K\n",
      "Receiving objects: 100% (196/196), 652.22 KiB | 4.41 MiB/s, done.\n",
      "Resolving deltas: 100% (98/98), done.\n",
      "/aiffel/aiffel/aiffel_project/KoGPT2\n",
      "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# https://opensourcelibs.com/lib/kogpt2-chatbot#architecture\n",
    "# 위 링크 코드를 따라하며 songys 챗봇 데이터셋과 pretrained ko-gpt2를 활용해 챗봇을 구현할 수 있었다.\n",
    "# 아래 코드는 모두 위 링크에서 가져왔다.\n",
    "\n",
    "!pip3 install mxnet-cu102\n",
    "!pip3 install gluonnlp sentencepiece pandas torch transformers pytorch_lightning \n",
    "\n",
    "!git clone https://github.com/SKT-AI/KoGPT2.git\n",
    "%cd KoGPT2\n",
    "!pip install .\n",
    "\n",
    "!git clone --recurse-submodules https://github.com/haven-jeon/KoGPT2-chatbot.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "substantial-craft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/aiffel_project/KoGPT2-chatbot\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: pytorch_lightning==1.2.10 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.2.10)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.9.0+cu111)\n",
      "Requirement already satisfied: transformers==4.5.1 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (4.5.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.18.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (4.56.0)\n",
      "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: torchmetrics==0.2.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.19.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (20.8)\n",
      "Requirement already satisfied: fsspec[http]>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (2021.8.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (2020.11.13)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (0.0.43)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (2.25.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->-r requirements.txt (line 3)) (3.7.4.3)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.7.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.4.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.32.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.7.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.14.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (4.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (4.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2020.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (20.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.1->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 4)) (7.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "2021-09-09 12:41:55.474290: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=96, benchmark=False, chat=False, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=5e-05, max_epochs=2, max_len=32, max_steps=None, min_epochs=None, min_steps=None, model_params='model_chp/model_-last.ckpt', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sentiment='0', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tpu_cores=None, track_grad_norm=-1, train=True, truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
      "INFO:filelock:Lock 140392648451344 acquired on /aiffel/.cache/huggingface/transformers/13bb826cf24517d7849a701e02452715a67c5e560142be3d4735442b2a545809.6b384eec6effdd44287f67715cd55bd0dff2cf846d843b932b43ba7b632b8b1e.lock\n",
      "Downloading: 100%|█████████████████████████| 1.00k/1.00k [00:00<00:00, 1.54MB/s]\n",
      "INFO:filelock:Lock 140392648451344 released on /aiffel/.cache/huggingface/transformers/13bb826cf24517d7849a701e02452715a67c5e560142be3d4735442b2a545809.6b384eec6effdd44287f67715cd55bd0dff2cf846d843b932b43ba7b632b8b1e.lock\n",
      "INFO:filelock:Lock 140392648451344 acquired on /aiffel/.cache/huggingface/transformers/495b405e3742953dbcc56685d1560fa02a2d86fc50b891868990a4471b06c934.4ebf112d34c2c8fc657866680005d92d21859c52c0ef5e941fa640129b2f8f88.lock\n",
      "Downloading: 100%|███████████████████████████| 513M/513M [00:09<00:00, 55.3MB/s]\n",
      "INFO:filelock:Lock 140392648451344 released on /aiffel/.cache/huggingface/transformers/495b405e3742953dbcc56685d1560fa02a2d86fc50b891868990a4471b06c934.4ebf112d34c2c8fc657866680005d92d21859c52c0ef5e941fa640129b2f8f88.lock\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | kogpt2        | GPT2LMHeadModel  | 125 M \n",
      "1 | loss_function | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "125 M     Trainable params\n",
      "0         Non-trainable params\n",
      "125 M     Total params\n",
      "500.656   Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                          | 0/134 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"train_torch.py\", line 238, in <module>\n",
      "    trainer.fit(model)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "    self.dispatch()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "    self.accelerator.start_training(self)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "    self.training_type_plugin.start_training(trainer)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "    self._results = trainer.run_train()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\n",
      "    self.train_loop.run_training_epoch()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 484, in run_training_epoch\n",
      "    for batch_idx, (batch, is_last_batch) in train_dataloader:\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/profiler/profilers.py\", line 82, in profile_iterable\n",
      "    value = next(iterator)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 47, in _with_is_last\n",
      "    last = next(it)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/supporters.py\", line 470, in __next__\n",
      "    return self.request_next_batch(self.loader_iters)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/supporters.py\", line 484, in request_next_batch\n",
      "    return apply_to_collection(loader_iters, Iterator, next)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py\", line 84, in apply_to_collection\n",
      "    return function(data, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1229, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/_utils.py\", line 425, in reraise\n",
      "    raise self.exc_type(msg)\n",
      "KeyError: Caught KeyError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3080, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 4554, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 4562, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'Q'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"train_torch.py\", line 72, in __getitem__\n",
      "    q = turn['Q']\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/core/series.py\", line 824, in __getitem__\n",
      "    return self._get_value(key)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/core/series.py\", line 932, in _get_value\n",
      "    loc = self.index.get_loc(label)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3082, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'Q'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd /aiffel/aiffel/aiffel_project/KoGPT2-chatbot\n",
    "!pip install -r requirements.txt\n",
    "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --train --gpus 1 --max_epochs 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "interim-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-09 12:43:31.860416: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=96, benchmark=False, chat=True, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=5e-05, max_epochs=None, max_len=32, max_steps=None, min_epochs=None, min_steps=None, model_params='model_chp/model_-last.ckpt', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sentiment='0', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tpu_cores=None, track_grad_norm=-1, train=False, truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
      "Traceback (most recent call last):\n",
      "  File \"train_torch.py\", line 241, in <module>\n",
      "    model = KoGPT2Chat.load_from_checkpoint(args.model_params)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/core/saving.py\", line 135, in load_from_checkpoint\n",
      "    checkpoint = pl_load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 44, in load\n",
      "    with fs.open(path_or_url, \"rb\") as f:\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fsspec/spec.py\", line 984, in open\n",
      "    **kwargs,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fsspec/implementations/local.py\", line 152, in _open\n",
      "    return LocalFileOpener(path, mode, fs=self, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fsspec/implementations/local.py\", line 243, in __init__\n",
      "    self._open()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fsspec/implementations/local.py\", line 248, in _open\n",
      "    self.f = open(self.path, mode=self.mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/aiffel/aiffel/aiffel_project/KoGPT2-chatbot/model_chp/model_-last.ckpt'\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --gpus 1 --chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-development",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
