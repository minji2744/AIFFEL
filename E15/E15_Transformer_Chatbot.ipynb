{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "E15_Transformer_Chatbot.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "southeast-fashion"
      },
      "source": [
        "#  E15. 트랜스포머로 챗봇 만들기\n",
        "- [songys github](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)에서 데이터셋을 가져와서 한글 질문에 대한 답변을 트랜스포머 모델에 학습한 후 챗봇을 만들어본다.\n",
        "- AI 허브의 정신건강 상담 데이터셋을 추가로 활용. [AI 허브 데이터셋](https://aihub.or.kr/keti_data_board/language_intelligence)"
      ],
      "id": "southeast-fashion"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gentle-african"
      },
      "source": [
        "**목차**\n",
        "1. 데이터 로드 및 전처리\n",
        "2. SubwordTextEncoder\n",
        "3. 모델 구성하기\n",
        "4. 모델 평가하기\n",
        "5. KoGPT2로 챗봇 만들어보기\n",
        "6. 회고"
      ],
      "id": "gentle-african"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ancient-triple"
      },
      "source": [
        "# 필요한 모듈 import\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "id": "ancient-triple",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "increasing-freeware",
        "outputId": "156ae9bb-5963-422a-d008-333314b746d1"
      },
      "source": [
        "!pip install openpyxl"
      ],
      "id": "increasing-freeware",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cLATYVindSQ",
        "outputId": "4525efd1-ce3d-4b2e-f633-b84d5d71769c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "2cLATYVindSQ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mobile-telephone"
      },
      "source": [
        "### 1. 데이터 로드 및 전처리"
      ],
      "id": "mobile-telephone"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "educated-brand",
        "outputId": "5addfe4b-1780-4254-fd59-f757348d3acd"
      },
      "source": [
        "path_to_data = '/content/drive/MyDrive/data/e15/ChatbotData.csv'\n",
        "\n",
        "chat = pd.read_csv(path_to_data)\n",
        "chat.head(20)  # 20개 샘플을 확인하여 전처리에 대한 아이디어 얻기"
      ],
      "id": "educated-brand",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SD카드 망가졌어</td>\n",
              "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>SD카드 안돼</td>\n",
              "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>SNS 맞팔 왜 안하지ㅠㅠ</td>\n",
              "      <td>잘 모르고 있을 수도 있어요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>SNS 시간낭비인 거 아는데 매일 하는 중</td>\n",
              "      <td>시간을 정하고 해보세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SNS 시간낭비인데 자꾸 보게됨</td>\n",
              "      <td>시간을 정하고 해보세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SNS보면 나만 빼고 다 행복해보여</td>\n",
              "      <td>자랑하는 자리니까요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>가끔 궁금해</td>\n",
              "      <td>그 사람도 그럴 거예요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>가끔 뭐하는지 궁금해</td>\n",
              "      <td>그 사람도 그럴 거예요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>가끔은 혼자인게 좋다</td>\n",
              "      <td>혼자를 즐기세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>가난한 자의 설움</td>\n",
              "      <td>돈은 다시 들어올 거예요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>가만 있어도 땀난다</td>\n",
              "      <td>땀을 식혀주세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>가상화폐 쫄딱 망함</td>\n",
              "      <td>어서 잊고 새출발 하세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>가스불 켜고 나갔어</td>\n",
              "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>가스불 켜놓고 나온거 같아</td>\n",
              "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>가스비 너무 많이 나왔다.</td>\n",
              "      <td>다음 달에는 더 절약해봐요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          Q                    A  label\n",
              "0                    12시 땡!           하루가 또 가네요.      0\n",
              "1               1지망 학교 떨어졌어            위로해 드립니다.      0\n",
              "2              3박4일 놀러가고 싶다          여행은 언제나 좋죠.      0\n",
              "3           3박4일 정도 놀러가고 싶다          여행은 언제나 좋죠.      0\n",
              "4                   PPL 심하네           눈살이 찌푸려지죠.      0\n",
              "5                 SD카드 망가졌어   다시 새로 사는 게 마음 편해요.      0\n",
              "6                   SD카드 안돼   다시 새로 사는 게 마음 편해요.      0\n",
              "7            SNS 맞팔 왜 안하지ㅠㅠ     잘 모르고 있을 수도 있어요.      0\n",
              "8   SNS 시간낭비인 거 아는데 매일 하는 중        시간을 정하고 해보세요.      0\n",
              "9         SNS 시간낭비인데 자꾸 보게됨        시간을 정하고 해보세요.      0\n",
              "10      SNS보면 나만 빼고 다 행복해보여          자랑하는 자리니까요.      0\n",
              "11                   가끔 궁금해        그 사람도 그럴 거예요.      0\n",
              "12              가끔 뭐하는지 궁금해        그 사람도 그럴 거예요.      0\n",
              "13              가끔은 혼자인게 좋다            혼자를 즐기세요.      0\n",
              "14                가난한 자의 설움       돈은 다시 들어올 거예요.      0\n",
              "15               가만 있어도 땀난다            땀을 식혀주세요.      0\n",
              "16               가상화폐 쫄딱 망함       어서 잊고 새출발 하세요.      0\n",
              "17               가스불 켜고 나갔어  빨리 집에 돌아가서 끄고 나오세요.      0\n",
              "18           가스불 켜놓고 나온거 같아  빨리 집에 돌아가서 끄고 나오세요.      0\n",
              "19           가스비 너무 많이 나왔다.      다음 달에는 더 절약해봐요.      0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awful-spell"
      },
      "source": [
        "- ChatbotData.csv 파일을 읽어보니, chat['Q']에는 질문이, chat['A']에는 답변이 쌍을 이룬 데이터이다. 이를 활용해서 병렬데이터를 구축하고 학습을 시킨다."
      ],
      "id": "awful-spell"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "second-conflict",
        "outputId": "74013353-b919-4640-ee57-c4e6f69bc655"
      },
      "source": [
        "print(chat.shape)"
      ],
      "id": "second-conflict",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11823, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dutch-gender",
        "outputId": "8e9e96cb-f7d1-422f-823e-b4ac34d2f320"
      },
      "source": [
        "# 고유 라벨 확인\n",
        "chat['label'].unique()"
      ],
      "id": "dutch-gender",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "incorporate-tiffany",
        "outputId": "308ef3c1-8b48-4015-8f37-2f3e2e62138d"
      },
      "source": [
        "chat[chat['label'] == 1]"
      ],
      "id": "incorporate-tiffany",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5290</th>\n",
              "      <td>1000일 만난 여자친구와 이별</td>\n",
              "      <td>더 오래 만날 사람 만날 거예요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5291</th>\n",
              "      <td>10년 연애. 헤어졌습니다.</td>\n",
              "      <td>더 공허함이 크시겠네요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5292</th>\n",
              "      <td>10년 연애사 되돌아보니 다 부질없네</td>\n",
              "      <td>더 좋은 사람 만나실 거예요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5293</th>\n",
              "      <td>10년 연예의끝</td>\n",
              "      <td>더 마음이 허하겠어요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5294</th>\n",
              "      <td>10년만나다 헤어지네</td>\n",
              "      <td>충분히 슬퍼하고 충분히 아파하다가 이겨내세요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8855</th>\n",
              "      <td>힘듭니다.</td>\n",
              "      <td>어떤 말도 위로가 되지 않겠지만 힘내세요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8856</th>\n",
              "      <td>힘이 될런지 모르겠지만</td>\n",
              "      <td>어떤 말도 위로가 되지 않겠지만 힘내세요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8857</th>\n",
              "      <td>힘이 드네</td>\n",
              "      <td>어떤 말도 위로가 되지 않겠지만 힘내세요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8858</th>\n",
              "      <td>힘이 없어</td>\n",
              "      <td>힘내세요!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8859</th>\n",
              "      <td>힘이드네.여자들이란.</td>\n",
              "      <td>어떤 말도 위로가 되지 않겠지만 힘내세요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3570 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Q                          A  label\n",
              "5290     1000일 만난 여자친구와 이별         더 오래 만날 사람 만날 거예요.      1\n",
              "5291       10년 연애. 헤어졌습니다.              더 공허함이 크시겠네요.      1\n",
              "5292  10년 연애사 되돌아보니 다 부질없네           더 좋은 사람 만나실 거예요.      1\n",
              "5293              10년 연예의끝               더 마음이 허하겠어요.      1\n",
              "5294           10년만나다 헤어지네  충분히 슬퍼하고 충분히 아파하다가 이겨내세요.      1\n",
              "...                    ...                        ...    ...\n",
              "8855                 힘듭니다.    어떤 말도 위로가 되지 않겠지만 힘내세요.      1\n",
              "8856          힘이 될런지 모르겠지만    어떤 말도 위로가 되지 않겠지만 힘내세요.      1\n",
              "8857                 힘이 드네    어떤 말도 위로가 되지 않겠지만 힘내세요.      1\n",
              "8858                 힘이 없어                      힘내세요!      1\n",
              "8859           힘이드네.여자들이란.    어떤 말도 위로가 되지 않겠지만 힘내세요.      1\n",
              "\n",
              "[3570 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "illegal-animal",
        "outputId": "50c0ce83-0b4c-434d-d16d-3498608410fd"
      },
      "source": [
        "chat[chat['label'] == 2]"
      ],
      "id": "illegal-animal",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8860</th>\n",
              "      <td>짝사랑만큼 고통스러운 건 없겠지.</td>\n",
              "      <td>짝사랑 만큼 감정소모가 큰 건 없을 거예요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8861</th>\n",
              "      <td>1년 넘게 만났는데 지금도 불타올라</td>\n",
              "      <td>정열적인 사랑을 하고 있나봐요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8862</th>\n",
              "      <td>1년 동거 중인데 계속 좋아</td>\n",
              "      <td>서로 깊게 알게되면서 더 좋아졌나봅니다.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8863</th>\n",
              "      <td>1년 동거하고 결혼했어</td>\n",
              "      <td>축하합니다!</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8864</th>\n",
              "      <td>1년 만났는데도 그 사람에 대해 잘 모르겠어</td>\n",
              "      <td>더 만나보세요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11818</th>\n",
              "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
              "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11819</th>\n",
              "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
              "      <td>훔쳐보는 거 티나나봐요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11820</th>\n",
              "      <td>흑기사 해주는 짝남.</td>\n",
              "      <td>설렜겠어요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11821</th>\n",
              "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
              "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11822</th>\n",
              "      <td>힘들어서 결혼할까봐</td>\n",
              "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2963 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Q                         A  label\n",
              "8860         짝사랑만큼 고통스러운 건 없겠지.  짝사랑 만큼 감정소모가 큰 건 없을 거예요.      2\n",
              "8861        1년 넘게 만났는데 지금도 불타올라         정열적인 사랑을 하고 있나봐요.      2\n",
              "8862            1년 동거 중인데 계속 좋아    서로 깊게 알게되면서 더 좋아졌나봅니다.      2\n",
              "8863               1년 동거하고 결혼했어                    축하합니다!      2\n",
              "8864   1년 만났는데도 그 사람에 대해 잘 모르겠어                  더 만나보세요.      2\n",
              "...                         ...                       ...    ...\n",
              "11818            훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
              "11819            훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
              "11820               흑기사 해주는 짝남.                    설렜겠어요.      2\n",
              "11821   힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
              "11822                힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
              "\n",
              "[2963 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "special-affect"
      },
      "source": [
        "- 데이터 제작자는 라벨에 대해 이렇게 설명하고 있습니다 :  일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링"
      ],
      "id": "special-affect"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sticky-liechtenstein"
      },
      "source": [
        "# AI 허브에서 정신건강 대화 데이터셋을 가져오기\n",
        "data_path = '/content/drive/MyDrive/data/e15/wellness_1.xlsx'\n",
        "wellness = pd.read_excel(data_path, engine='openpyxl')"
      ],
      "id": "sticky-liechtenstein",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "great-bacteria",
        "outputId": "7fae02b4-3aae-4454-e774-975af62e3f5b"
      },
      "source": [
        "wellness.head(20)"
      ],
      "id": "great-bacteria",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>구분</th>\n",
              "      <th>유저</th>\n",
              "      <th>챗봇</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.</td>\n",
              "      <td>감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>더 이상 내 감정을 내가 컨트롤 못 하겠어.</td>\n",
              "      <td>저도 그 기분 이해해요. 많이 힘드시죠?</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>하루종일 오르락내리락 롤러코스터 타는 기분이에요.</td>\n",
              "      <td>그럴 때는 밥은 잘 먹었는지, 잠은 잘 잤는지 체크해보는 것도 좋아요.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>꼭 롤러코스터 타는 것 같아요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>롤러코스터 타는 것처럼 기분이 왔다 갔다 해요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>작년 가을부터 감정조절이 잘 안 되는 거 같아.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>예전에 비해서 인내심이 너무 짧아진 거 같아.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>더 이상 혼자서는 감정조절을 못하겠어.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>점점 나 자신을 컨트롤하지 못하는 기분이야.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>나도 이러기 싫은데 내 마음대로 안돼.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>맨정신일 때는 저를 주체할 수 가 없었거든요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>주체가 안 돼.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>이렇게 쌓이고 쌓이다 나중에 확 터지거든요. 진짜 걷잡을 수 없이요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>근데 감정을 다스리지 못해 욱하기도하고.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>감정/감정조절이상</td>\n",
              "      <td>순간순간 감정조절을 못해요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>감정/감정조절이상/화</td>\n",
              "      <td>평소 다른 일을 할 때도 비슷해요. 생각한대로 안되면 화가 나고…그런 상황이 지속되...</td>\n",
              "      <td>화가 폭발할 것 같을 때는 그 자리를 피하는 것도 좋은 방법이라고 생각해요.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>감정/감정조절이상/화</td>\n",
              "      <td>예전보다 화내는 게 과격해진 거 같아.</td>\n",
              "      <td>정말 힘드시겠어요. 화는 남에게도 스스로에게도 상처를 주잖아요.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>감정/감정조절이상/화</td>\n",
              "      <td>화가 안 참아져.</td>\n",
              "      <td>화가 너무 많이 날 때는 심호흡을 해보는 게 어떨까요? 씁- 후-</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>감정/감정조절이상/화</td>\n",
              "      <td>근데 다음에 또 그러면 또 화가 나고… 모르겠어요. 제 감정이 통제가 안 돼요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>감정/감정조절이상/화</td>\n",
              "      <td>마음은 안 그런데 화를 낼 때 더 불같이 내게 되기도 하고..</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             구분  ... Unnamed: 3\n",
              "0     감정/감정조절이상  ...        NaN\n",
              "1     감정/감정조절이상  ...        NaN\n",
              "2     감정/감정조절이상  ...        NaN\n",
              "3     감정/감정조절이상  ...        NaN\n",
              "4     감정/감정조절이상  ...        NaN\n",
              "5     감정/감정조절이상  ...        NaN\n",
              "6     감정/감정조절이상  ...        NaN\n",
              "7     감정/감정조절이상  ...        NaN\n",
              "8     감정/감정조절이상  ...        NaN\n",
              "9     감정/감정조절이상  ...        NaN\n",
              "10    감정/감정조절이상  ...        NaN\n",
              "11    감정/감정조절이상  ...        NaN\n",
              "12    감정/감정조절이상  ...        NaN\n",
              "13    감정/감정조절이상  ...        NaN\n",
              "14    감정/감정조절이상  ...        NaN\n",
              "15  감정/감정조절이상/화  ...        NaN\n",
              "16  감정/감정조절이상/화  ...        NaN\n",
              "17  감정/감정조절이상/화  ...        NaN\n",
              "18  감정/감정조절이상/화  ...        NaN\n",
              "19  감정/감정조절이상/화  ...        NaN\n",
              "\n",
              "[20 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sharing-mailman"
      },
      "source": [
        "- wellness 데이터셋을 chat 데이터셋과 concatenate 해서 조금 더 풍부하게 데이터셋을 구성해보고자 한다."
      ],
      "id": "sharing-mailman"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "authentic-salvation",
        "outputId": "69ea0baa-d021-4fcc-e88b-a67c76ad8259"
      },
      "source": [
        "# 결측치 확인\n",
        "wellness.isnull().sum()"
      ],
      "id": "authentic-salvation",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "구분               0\n",
              "유저               0\n",
              "챗봇            4197\n",
              "Unnamed: 3    5231\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fluid-variety",
        "outputId": "655fbb04-f7c2-4e22-8f38-378792b322a7"
      },
      "source": [
        "print(wellness.shape)"
      ],
      "id": "fluid-variety",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5231, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "funded-cycling",
        "outputId": "fbb49b5a-8a54-4ccf-8581-c75820f72014"
      },
      "source": [
        "wellness = wellness.drop(['Unnamed: 3'], axis=1)\n",
        "wellness.dropna(inplace=True)\n",
        "print(wellness.shape)"
      ],
      "id": "funded-cycling",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1034, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acting-beverage",
        "outputId": "120f8898-80b2-49d8-c2ed-e38c9be5c3d0"
      },
      "source": [
        "wellness['구분'].unique()"
      ],
      "id": "acting-beverage",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['감정/감정조절이상', '감정/감정조절이상/화', '감정/걱정', '감정/걱정/건강문제', '감정/걱정/건강염려',\n",
              "       '감정/걱정/경제적문제', '감정/걱정/미래', '감정/걱정/불면', '감정/걱정/암', '감정/걱정/자녀',\n",
              "       '감정/걱정/주변평가', '감정/걱정/증상재발', '감정/고독감', '감정/곤혹감', '감정/공포', '감정/공포/새',\n",
              "       '감정/공허감', '감정/과민반응', '감정/괴로움', '감정/기분저하', '감정/기시감', '감정/긴장',\n",
              "       '감정/눈물', '감정/답답', '감정/답답/사람많은곳', '감정/당황', '감정/두려움', '감정/두려움/운전',\n",
              "       '감정/두려움/자동차', '감정/멍함', '감정/모호함', '감정/무력감', '감정/무미건조', '감정/무서움',\n",
              "       '감정/미안함/자녀', '감정/미움', '감정/배신감', '감정/부정적사고', '감정/분노', '감정/불만',\n",
              "       '감정/불신', '감정/불안감', '감정/불안감/긴장', '감정/불안감/미래', '감정/불안감/증상재발',\n",
              "       '감정/불안감/초조함', '감정/불쾌감', '감정/불편감', '감정/비관적', '감정/살인욕구', '감정/생각',\n",
              "       '감정/서운함', '감정/속상함', '감정/슬픔', '감정/신경쓰임', '감정/심란', '감정/억울함',\n",
              "       '감정/예민함', '감정/외로움', '감정/우울감', '감정/우울감/눈물', '감정/우울감/증상재발',\n",
              "       '감정/우울감/증상지속', '감정/의기소침', '감정/의기소침/자격지심', '감정/의욕상실', '감정/의욕상실/무기력',\n",
              "       '감정/자괴감', '감정/자살충동', '감정/자신감저하', '감정/자존감저하', '감정/절망감', '감정/좌절',\n",
              "       '감정/죄책감', '감정/즐거움', '감정/짜증', '감정/창피함', '감정/초조함', '감정/충격',\n",
              "       '감정/통제력상실', '감정/허무함', '감정/화', '감정/후회', '감정/후회/결혼', '감정/힘듦',\n",
              "       '감정/힘듦/스트레스', '감정/힘듦/지침', '내원이유/상담', '내원이유/의사소견', '내원이유/치료', '모호함',\n",
              "       '배경/가족', '배경/가족/갈등', '배경/가족/무관심', '배경/건강문제', '배경/건강문제/갑상선',\n",
              "       '배경/건강문제/다이어트', '배경/건강문제/다이어트/스트레스', '배경/건강문제/생리불순', '배경/건강문제/수술',\n",
              "       '배경/건강문제/알레르기', '배경/건강문제/항암', '배경/결혼', '배경/결혼/미혼', '배경/경제적문제',\n",
              "       '배경/경제적문제/가난', '배경/경제적문제/빚', '배경/공부', '배경/공부/부진', '배경/군대/군입대',\n",
              "       '배경/귀국', '배경/남자친구', '배경/남자친구/고민/없음', '배경/남자친구/동경', '배경/남자친구/없음',\n",
              "       '배경/남자친구/이별', '배경/남자친구/집착', '배경/남자친구/짧은교제', '배경/남편', '배경/남편/갈등',\n",
              "       '배경/남편/경제적문제', '배경/남편/과음', '배경/남편/관계소원', '배경/남편/관계양호', '배경/남편/다툼',\n",
              "       '배경/남편/무관심', '배경/남편/바람', '배경/남편/사업', '배경/남편/소통불가', '배경/남편/의심',\n",
              "       '배경/남편/의지', '배경/남편/폭력', '배경/대인관계', '배경/대인관계/갈등', '배경/대인관계/양호',\n",
              "       '배경/대인관계/협소', '배경/대학', '배경/대학/실패', '배경/대학/실패/재수', '배경/대학/입학',\n",
              "       '배경/대학/재수', '배경/대학/휴학', '배경/문제', '배경/문제/과음', '배경/문제/머리카락/털뽑기',\n",
              "       '배경/문제/불면', '배경/문제/불안감/소변', '배경/문제/불편감/옷', '배경/문제/소변',\n",
              "       '배경/문제/알코올의존', '배경/부모', '배경/부모/가출/아버지', '배경/부모/갈등', '배경/부모/갈등/아버지',\n",
              "       '배경/부모/관계소원', '배경/부모/무관심', '배경/부모/싸움', '배경/부모/아버지', '배경/부모/아버지/죽음',\n",
              "       '배경/부모/아버지/폭력', '배경/부모/어머니/죽음', '배경/부모/이혼', '배경/사고', '배경/사고/교통사고',\n",
              "       '배경/사업', '배경/사업/경제적문제/실패', '배경/사업/남편/동업자', '배경/사업/실패', '배경/생활',\n",
              "       '배경/생활/불가능/운전', '배경/생활/스트레스', '배경/생활/양호', '배경/생활/여행/해외',\n",
              "       '배경/생활/운동', '배경/생활/자연소멸/증상', '배경/생활/취미', '배경/생활/폭행/피해', '배경/생활/해외',\n",
              "       '배경/생활/혼자', '배경/생활/휴식', '배경/성격', '배경/성격/극단적', '배경/성격/급함',\n",
              "       '배경/성격/내성적', '배경/성격/소심', '배경/성격/예민함', '배경/성격/완벽추구', '배경/성격/욕심많음',\n",
              "       '배경/성격/자기중심적', '배경/성격/자립적', '배경/시댁', '배경/시댁/갈등', '배경/시댁/갈등/시어머니',\n",
              "       '배경/시댁/경제적문제', '배경/아르바이트', '배경/애완동물', '배경/애완동물/가족/갈등', '배경/어린시절',\n",
              "       '배경/어린시절/가난', '배경/여자친구', '배경/여자친구/관계소원', '배경/여자친구/동거',\n",
              "       '배경/여자친구/이별', '배경/연애', '배경/연애/이별', '배경/유학', '배경/육아/힘듦', '배경/음주',\n",
              "       '배경/음주/과음', '배경/음주/알코올의존', '배경/음주/애주가', '배경/음주/자주', '배경/이사',\n",
              "       '배경/이혼', '배경/임신', '배경/임신/낙태', '배경/자각/우울증', '배경/자각/정신질환', '배경/자녀',\n",
              "       '배경/전연인', '배경/종교', '배경/직장', '배경/직장/고민/퇴사', '배경/직장/과도한업무',\n",
              "       '배경/직장/반복/이직', '배경/직장/복직', '배경/직장/불만', '배경/직장/불만/업무', '배경/직장/스트레스',\n",
              "       '배경/직장/양호', '배경/직장/없음/흥미', '배경/직장/이직', '배경/직장/퇴사', '배경/직장/휴직',\n",
              "       '배경/진로', '배경/취업', '배경/취업/준비', '배경/취업/힘듦', '배경/친구', '배경/친구/관계소원',\n",
              "       '배경/친구/배신', '배경/친구/없음', '배경/타인/갈등', '배경/학교', '배경/학교/갈등/선생님',\n",
              "       '배경/학교/결석', '배경/학교/따돌림', '배경/학교/자퇴', '배경/학업', '배경/학업/부진',\n",
              "       '배경/학업/양호', '배경/학업/우수', '부가설명', '상태/양호', '상태/증상감소', '상태/증상지속',\n",
              "       '원인/없음', '자가치료/심리조절', '자가치료/심리조절/증상지속', '자가치료/운동', '자가치료/충분한휴식',\n",
              "       '증상/가슴답답', '증상/가슴떨림', '증상/가슴통증', '증상/건강염려', '증상/공격적성향', '증상/공황발작',\n",
              "       '증상/과대망상', '증상/과수면', '증상/기억력저하', '증상/기억력저하/집중력저하', '증상/기억상실',\n",
              "       '증상/기절', '증상/기절예기', '증상/대인기피', '증상/대화기피', '증상/두근거림', '증상/두근거림/불면',\n",
              "       '증상/두통', '증상/두통/불면', '증상/떨림', '증상/만성피로', '증상/메스꺼움', '증상/무기력',\n",
              "       '증상/무기력/은둔', '증상/무기력/의욕상실', '증상/반복사고', '증상/반복사고/트라우마', '증상/반복행동',\n",
              "       '증상/반복행동/대화', '증상/반복행동/문단속', '증상/반복행동/손씻기', '증상/반복행동/확인', '증상/발작',\n",
              "       '증상/불면', '증상/불면/불안감', '증상/불면/생각많음', '증상/불면/스트레스', '증상/불면/예민함',\n",
              "       '증상/불면/증상지속', '증상/불면/피로', '증상/생리불순', '증상/성격변화', '증상/성욕상승',\n",
              "       '증상/소화불량', '증상/속쓰림', '증상/시력저하', '증상/식욕저하', '증상/식욕저하/불면',\n",
              "       '증상/식욕저하/체중감소', '증상/신체이상/목', '증상/악몽', '증상/알코올의존', '증상/어지러움',\n",
              "       '증상/은둔', '증상/이명', '증상/이인감', '증상/인지기능저하', '증상/자살시도', '증상/자해',\n",
              "       '증상/저림현상/발/손', '증상/죽음공포', '증상/죽음공포/호흡곤란', '증상/집중력저하', '증상/체력저하',\n",
              "       '증상/체중감소', '증상/체중증가', '증상/컨디션저조', '증상/통증', '증상/통증/목', '증상/통증/전신',\n",
              "       '증상/통증/허리', '증상/편두통', '증상/폭식', '증상/폭식/스트레스/해소', '증상/피로',\n",
              "       '증상/피로/불면', '증상/피해망상', '증상/피해망상/감시', '증상/피해망상/감시/남편', '증상/피해망상/남편',\n",
              "       '증상/피해망상/도청', '증상/호흡곤란', '증상/호흡곤란/가슴답답', '증상/환각', '증상/환청',\n",
              "       '증상/힘빠짐', '치료이력/검사', '치료이력/검사/이상없음', '치료이력/병원내원',\n",
              "       '치료이력/병원내원/검사/이상없음', '치료이력/병원내원/복약', '치료이력/병원내원/이상없음', '치료이력/응급실',\n",
              "       '현재상태/증상악화'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "demographic-surface"
      },
      "source": [
        "wellness = wellness[['유저', '챗봇']]\n",
        "wellness = wellness.rename({'유저' : 'Q', '챗봇' : 'A'}, axis = 1)\n",
        "wellness['label'] = np.ones((wellness.shape[0],), dtype=int)  # 부정적인 내용의 데이터이므로 라벨을 1로 해줍니다."
      ],
      "id": "demographic-surface",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "induced-beverage",
        "outputId": "a4ae924d-178e-47b6-bef2-51602d83bdfc"
      },
      "source": [
        "display(wellness)"
      ],
      "id": "induced-beverage",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.</td>\n",
              "      <td>감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>더 이상 내 감정을 내가 컨트롤 못 하겠어.</td>\n",
              "      <td>저도 그 기분 이해해요. 많이 힘드시죠?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>하루종일 오르락내리락 롤러코스터 타는 기분이에요.</td>\n",
              "      <td>그럴 때는 밥은 잘 먹었는지, 잠은 잘 잤는지 체크해보는 것도 좋아요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>평소 다른 일을 할 때도 비슷해요. 생각한대로 안되면 화가 나고…그런 상황이 지속되...</td>\n",
              "      <td>화가 폭발할 것 같을 때는 그 자리를 피하는 것도 좋은 방법이라고 생각해요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>예전보다 화내는 게 과격해진 거 같아.</td>\n",
              "      <td>정말 힘드시겠어요. 화는 남에게도 스스로에게도 상처를 주잖아요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5196</th>\n",
              "      <td>그 사람이 응급실 의사한테 뭐라고 속닥거리니까, 저보고 갑자기 응급처치 끝났다고, ...</td>\n",
              "      <td>응급실이 있어서 다행이네요. 큰 문제는 없으신 거죠?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5197</th>\n",
              "      <td>파편이 튀어서 그 때 저도 응급실 가서 치료 받기도 했고…</td>\n",
              "      <td>응급실에 가셨다니 정말 놀랐어요. 아무 문제 없으신가요? 걱정 되네요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5213</th>\n",
              "      <td>지금 상태가 너무 안 좋아서 학교 안 나가고 있어요.</td>\n",
              "      <td>상태가 더 안 좋아지셨군요. 걱정이 되네요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5214</th>\n",
              "      <td>진짜 심해진 거 같긴 해요.</td>\n",
              "      <td>정말 힘드시겠어요. 지금도 증상이 심하신가요?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5215</th>\n",
              "      <td>그런데 증상이 나빠진 거 같아.</td>\n",
              "      <td>너무 심하시면 병원을 다시 가보는 건 어떨까요?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1034 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      Q  ... label\n",
              "0       제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.  ...     1\n",
              "1                              더 이상 내 감정을 내가 컨트롤 못 하겠어.  ...     1\n",
              "2                           하루종일 오르락내리락 롤러코스터 타는 기분이에요.  ...     1\n",
              "15    평소 다른 일을 할 때도 비슷해요. 생각한대로 안되면 화가 나고…그런 상황이 지속되...  ...     1\n",
              "16                                예전보다 화내는 게 과격해진 거 같아.  ...     1\n",
              "...                                                 ...  ...   ...\n",
              "5196  그 사람이 응급실 의사한테 뭐라고 속닥거리니까, 저보고 갑자기 응급처치 끝났다고, ...  ...     1\n",
              "5197                   파편이 튀어서 그 때 저도 응급실 가서 치료 받기도 했고…  ...     1\n",
              "5213                     지금 상태가 너무 안 좋아서 학교 안 나가고 있어요.   ...     1\n",
              "5214                                   진짜 심해진 거 같긴 해요.   ...     1\n",
              "5215                                 그런데 증상이 나빠진 거 같아.   ...     1\n",
              "\n",
              "[1034 rows x 3 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aggressive-hawaiian",
        "outputId": "fd3a859b-14d8-4929-8f9b-618b8031f615"
      },
      "source": [
        "chat = pd.concat([chat, wellness])\n",
        "print(chat.shape) # chat과 wellness 합하기"
      ],
      "id": "aggressive-hawaiian",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12857, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "8FgTVrp7rf_J",
        "outputId": "ad359bde-9ccc-4b61-bcb6-7157388939dd"
      },
      "source": [
        "display(chat)"
      ],
      "id": "8FgTVrp7rf_J",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5196</th>\n",
              "      <td>그 사람이 응급실 의사한테 뭐라고 속닥거리니까, 저보고 갑자기 응급처치 끝났다고, ...</td>\n",
              "      <td>응급실이 있어서 다행이네요. 큰 문제는 없으신 거죠?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5197</th>\n",
              "      <td>파편이 튀어서 그 때 저도 응급실 가서 치료 받기도 했고…</td>\n",
              "      <td>응급실에 가셨다니 정말 놀랐어요. 아무 문제 없으신가요? 걱정 되네요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5213</th>\n",
              "      <td>지금 상태가 너무 안 좋아서 학교 안 나가고 있어요.</td>\n",
              "      <td>상태가 더 안 좋아지셨군요. 걱정이 되네요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5214</th>\n",
              "      <td>진짜 심해진 거 같긴 해요.</td>\n",
              "      <td>정말 힘드시겠어요. 지금도 증상이 심하신가요?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5215</th>\n",
              "      <td>그런데 증상이 나빠진 거 같아.</td>\n",
              "      <td>너무 심하시면 병원을 다시 가보는 건 어떨까요?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12857 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      Q  ... label\n",
              "0                                                12시 땡!  ...     0\n",
              "1                                           1지망 학교 떨어졌어  ...     0\n",
              "2                                          3박4일 놀러가고 싶다  ...     0\n",
              "3                                       3박4일 정도 놀러가고 싶다  ...     0\n",
              "4                                               PPL 심하네  ...     0\n",
              "...                                                 ...  ...   ...\n",
              "5196  그 사람이 응급실 의사한테 뭐라고 속닥거리니까, 저보고 갑자기 응급처치 끝났다고, ...  ...     1\n",
              "5197                   파편이 튀어서 그 때 저도 응급실 가서 치료 받기도 했고…  ...     1\n",
              "5213                     지금 상태가 너무 안 좋아서 학교 안 나가고 있어요.   ...     1\n",
              "5214                                   진짜 심해진 거 같긴 해요.   ...     1\n",
              "5215                                 그런데 증상이 나빠진 거 같아.   ...     1\n",
              "\n",
              "[12857 rows x 3 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "manufactured-yellow"
      },
      "source": [
        "**전처리 프로세스**\n",
        "> 1) 단어와 구두점 분리  \n",
        "> 2) 의미를 가진 단어로 영어, 한글, 숫자를 포함하기  \n",
        "> 3) 구두점, 영어, 한글, 숫자를 제외한 특수문자 등은 공백으로 처리하기"
      ],
      "id": "manufactured-yellow"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "german-hypothesis"
      },
      "source": [
        "# 전처리 함수\n",
        "def preprocess_sentence(sentence):\n",
        "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "  # (영어, 한글, 숫자, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
        "  sentence = re.sub(r\"[^a-zA-Z|ㄱ-ㅎ|ㅏ-ㅣ|가-힣|?.!,0-9]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "    \n",
        "  return sentence"
      ],
      "id": "german-hypothesis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nutritional-contest"
      },
      "source": [
        "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
        "def load_conversations(df):\n",
        "  inputs, outputs = [], []\n",
        "\n",
        "  questions = df['Q']\n",
        "  for line in questions:\n",
        "    inputs.append(preprocess_sentence(line))\n",
        "\n",
        "  answers = df['A']\n",
        "  for line in answers:\n",
        "    outputs.append(preprocess_sentence(line))    \n",
        "\n",
        "  return inputs, outputs"
      ],
      "id": "nutritional-contest",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adjustable-timer",
        "outputId": "12768faa-6b50-4d01-ab75-254f20fb65a2"
      },
      "source": [
        "questions, answers = load_conversations(chat)\n",
        "\n",
        "print('전체 샘플 수 :', len(questions))\n",
        "print('전체 샘플 수 :', len(answers))\n",
        "\n",
        "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[21]))\n",
        "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[21]))"
      ],
      "id": "adjustable-timer",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전체 샘플 수 : 12857\n",
            "전체 샘플 수 : 12857\n",
            "전처리 후의 22번째 질문 샘플: 가스비 장난 아님\n",
            "전처리 후의 22번째 답변 샘플: 다음 달에는 더 절약해봐요 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "popular-judgment"
      },
      "source": [
        "### 2. SubwordTextEncoder"
      ],
      "id": "popular-judgment"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acceptable-roberts",
        "outputId": "1be7df36-5123-4a9b-9638-4fa3823b1a58"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n",
        "\n",
        "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성. (Tensorflow 2.3.0 이상) (클라우드는 2.4 입니다)\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
        "print(\"슝=3 \")"
      ],
      "id": "acceptable-roberts",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\n",
            "슝=3 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plastic-brooklyn",
        "outputId": "e8454041-f628-4a63-c12c-0e84c9521358"
      },
      "source": [
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
        "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])\n",
        "\n",
        "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "print(VOCAB_SIZE)"
      ],
      "id": "plastic-brooklyn",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "START_TOKEN의 번호 : [7658]\n",
            "END_TOKEN의 번호 : [7659]\n",
            "7660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "useful-costume",
        "outputId": "63d9d0a2-9572-437d-85dc-d2f25af0df1c"
      },
      "source": [
        "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
        "# 각 토큰을 고유한 정수로 변환\n",
        "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
        "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
      ],
      "id": "useful-costume",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 21번째 질문 샘플: [5916, 676, 3217, 5317]\n",
            "정수 인코딩 후의 21번째 답변 샘플: [2256, 4000, 8, 13, 2858, 736, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fatty-cover",
        "outputId": "6d453428-f964-4572-b759-8a14fa8a09f3"
      },
      "source": [
        "MAX_LENGTH = 40\n",
        "\n",
        "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "  \n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "  \n",
        "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "print(\"슝=3\")"
      ],
      "id": "fatty-cover",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "specialized-louisville",
        "outputId": "eed6f445-9227-413c-dd28-137711c88bd6"
      },
      "source": [
        "questions, answers = tokenize_and_filter(questions, answers)\n",
        "print('단어장의 크기 :',(VOCAB_SIZE))\n",
        "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
        "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
      ],
      "id": "specialized-louisville",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단어장의 크기 : 7660\n",
            "필터링 후의 질문 샘플 개수: 12857\n",
            "필터링 후의 답변 샘플 개수: 12857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hispanic-cigarette",
        "outputId": "3d79194e-9d66-4ddf-eabe-809c1d3b0a8b"
      },
      "source": [
        "# 교사 강요\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
        "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "print(\"슝=3\")"
      ],
      "id": "hispanic-cigarette",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "secure-rochester"
      },
      "source": [
        "### 3. 모델 구성"
      ],
      "id": "secure-rochester"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geological-camera"
      },
      "source": [
        "[transformer](http://jalammar.github.io/illustrated-transformer/) 모델에 대한 설명은 링크를 참고했고, [\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf) 해당 논문에서 트랜스포머 모델의 구조를 가져왔습니다.\n",
        "\n",
        "![nn](transformer.png)"
      ],
      "id": "geological-camera"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enormous-backup",
        "outputId": "e75d53bc-35fe-4e79-84b3-6f24b280ac17"
      },
      "source": [
        "# 포지셔널 인코딩 레이어\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    # 각도 배열 생성\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "\n",
        "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    # sin과 cosine이 교차되도록 재배열\n",
        "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
        "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
        "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
        "\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "print(\"슝=3\")"
      ],
      "id": "enormous-backup",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bronze-campus",
        "outputId": "0871ea7e-20b8-420c-88f5-2174780c9a3b"
      },
      "source": [
        "# 스케일드 닷 프로덕트 어텐션 함수\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  # 가중치를 정규화\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # 패딩에 마스크 추가\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  # softmax적용\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
        "  output = tf.matmul(attention_weights, value)\n",
        "  return output\n",
        "\n",
        "print(\"슝=3\")"
      ],
      "id": "bronze-campus",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hollywood-petroleum",
        "outputId": "321f050d-9e7a-4140-c4aa-087a82e47081"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # Q, K, V에 각각 Dense를 적용합니다\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    # 스케일드 닷 프로덕트 어텐션 함수\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs\n",
        "print(\"슝=3\")"
      ],
      "id": "hollywood-petroleum",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bronze-lawrence"
      },
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "id": "bronze-lawrence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "genuine-storage",
        "outputId": "756e86f6-a8b5-47c0-b28b-3ec2a7065b61"
      },
      "source": [
        "# 인코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "print(\"슝=3\")"
      ],
      "id": "genuine-storage",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "private-chassis",
        "outputId": "98099224-f327-4f17-c5f5-947a7289e8ea"
      },
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  # num_layers만큼 쌓아올린 인코더의 층.\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "print(\"슝=3\")"
      ],
      "id": "private-chassis",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "documentary-soundtrack",
        "outputId": "d5c30e7d-89c1-4c22-a542-ba6375febb4e"
      },
      "source": [
        "# 디코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "\n",
        "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
        "  attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
        "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "  attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)\n",
        "print(\"슝=3\")"
      ],
      "id": "documentary-soundtrack",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "collected-patio",
        "outputId": "404fd611-355f-4e5c-dd23-3cc8002b1704"
      },
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name='decoder'):\n",
        "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "\n",
        "  # 패딩 마스크\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "  \n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = decoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name='decoder_layer_{}'.format(i),\n",
        "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)\n",
        "print(\"슝=3\")"
      ],
      "id": "collected-patio",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "limiting-greece",
        "outputId": "7de93335-2ae5-4656-acbb-628a7477053c"
      },
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "  # 인코더에서 패딩을 위한 마스크\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
        "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
        "  look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
        "  # 디코더에서 패딩을 위한 마스크\n",
        "  dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "  # 인코더\n",
        "  enc_outputs = encoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  # 디코더\n",
        "  dec_outputs = decoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "  # 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
        "print(\"슝=3\")"
      ],
      "id": "limiting-greece",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "periodic-settle",
        "outputId": "ea2e9b6b-36db-4a4d-d434-a1604e6b9e35"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 하이퍼파라미터\n",
        "NUM_LAYERS = 3 # 인코더와 디코더의 층의 개수\n",
        "D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
        "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
        "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
        "DROPOUT = 0.1 # 드롭아웃의 비율\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)\n",
        "\n",
        "model.summary()"
      ],
      "id": "periodic-settle",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "inputs (InputLayer)             [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Functional)            (None, None, 512)    8655872     inputs[0][0]                     \n",
            "                                                                 enc_padding_mask[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Functional)            (None, None, 512)    11810816    dec_inputs[0][0]                 \n",
            "                                                                 encoder[0][0]                    \n",
            "                                                                 look_ahead_mask[0][0]            \n",
            "                                                                 dec_padding_mask[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "outputs (Dense)                 (None, None, 7660)   3929580     decoder[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 24,396,268\n",
            "Trainable params: 24,396,268\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "immune-machinery",
        "outputId": "30aae5e0-9893-4bc6-936d-fb7c2c639b48"
      },
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  \n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)\n",
        "print(\"슝=3\")"
      ],
      "id": "immune-machinery",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "interested-gamma",
        "outputId": "c44dc787-96f4-425f-9ffe-f28a9d546760"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "print(\"슝=3\")"
      ],
      "id": "interested-gamma",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "governing-safety",
        "outputId": "979230f5-4edd-418a-c840-c764854daeeb"
      },
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "print(\"슝=3\")"
      ],
      "id": "governing-safety",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "precious-concentration",
        "outputId": "f026893d-67e4-4f42-b920-b17117868e3b"
      },
      "source": [
        "EPOCHS = 20\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "id": "precious-concentration",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "201/201 [==============================] - 65s 268ms/step - loss: 1.5926 - accuracy: 0.0192\n",
            "Epoch 2/20\n",
            "201/201 [==============================] - 55s 271ms/step - loss: 1.2179 - accuracy: 0.0490\n",
            "Epoch 3/20\n",
            "201/201 [==============================] - 55s 275ms/step - loss: 1.0804 - accuracy: 0.0525\n",
            "Epoch 4/20\n",
            "201/201 [==============================] - 55s 274ms/step - loss: 1.0093 - accuracy: 0.0572\n",
            "Epoch 5/20\n",
            "201/201 [==============================] - 55s 275ms/step - loss: 0.9289 - accuracy: 0.0620\n",
            "Epoch 6/20\n",
            "201/201 [==============================] - 56s 277ms/step - loss: 0.8499 - accuracy: 0.0690\n",
            "Epoch 7/20\n",
            "201/201 [==============================] - 55s 275ms/step - loss: 0.7573 - accuracy: 0.0790\n",
            "Epoch 8/20\n",
            "201/201 [==============================] - 55s 275ms/step - loss: 0.6602 - accuracy: 0.0896\n",
            "Epoch 9/20\n",
            "201/201 [==============================] - 55s 274ms/step - loss: 0.5608 - accuracy: 0.1013\n",
            "Epoch 10/20\n",
            "201/201 [==============================] - 55s 273ms/step - loss: 0.4615 - accuracy: 0.1148\n",
            "Epoch 11/20\n",
            "201/201 [==============================] - 56s 278ms/step - loss: 0.3730 - accuracy: 0.1288\n",
            "Epoch 12/20\n",
            "201/201 [==============================] - 55s 276ms/step - loss: 0.2847 - accuracy: 0.1397\n",
            "Epoch 13/20\n",
            "201/201 [==============================] - 55s 275ms/step - loss: 0.2109 - accuracy: 0.1525\n",
            "Epoch 14/20\n",
            "201/201 [==============================] - 55s 274ms/step - loss: 0.1545 - accuracy: 0.1629\n",
            "Epoch 15/20\n",
            "201/201 [==============================] - 55s 274ms/step - loss: 0.1119 - accuracy: 0.1706\n",
            "Epoch 16/20\n",
            "201/201 [==============================] - 55s 275ms/step - loss: 0.0840 - accuracy: 0.1739\n",
            "Epoch 17/20\n",
            "201/201 [==============================] - 55s 276ms/step - loss: 0.0702 - accuracy: 0.1766\n",
            "Epoch 18/20\n",
            "201/201 [==============================] - 55s 276ms/step - loss: 0.0641 - accuracy: 0.1784\n",
            "Epoch 19/20\n",
            "201/201 [==============================] - 55s 275ms/step - loss: 0.0602 - accuracy: 0.1781\n",
            "Epoch 20/20\n",
            "201/201 [==============================] - 55s 274ms/step - loss: 0.0606 - accuracy: 0.1782\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9b3ba43e50>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efficient-circus"
      },
      "source": [
        "## 4. 모델 평가하기"
      ],
      "id": "efficient-circus"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "statistical-boxing",
        "outputId": "c5230787-807d-4aae-92fc-cb213f334c7a"
      },
      "source": [
        "def decoder_inference(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
        "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
        "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
        "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  # 디코더의 인퍼런스 단계\n",
        "  for i in range(MAX_LENGTH):\n",
        "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
        "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
        "    predictions = predictions[:, -1:, :]\n",
        "\n",
        "    # 현재 예측한 단어의 정수\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
        "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
        "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output_sequence, axis=0)\n",
        "print(\"슝=3\")"
      ],
      "id": "statistical-boxing",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "public-heart",
        "outputId": "7a001789-6148-4769-f1d4-8466a33a2cea"
      },
      "source": [
        "def sentence_generation(sentence):\n",
        "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
        "  prediction = decoder_inference(sentence)\n",
        "\n",
        "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "  print('입력 : {}'.format(sentence))\n",
        "  print('출력 : {}'.format(predicted_sentence))\n",
        "\n",
        "  return predicted_sentence\n",
        "print(\"슝=3\")"
      ],
      "id": "public-heart",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3tNg1q-M4Pi"
      },
      "source": [
        "**출력 확인해보기**"
      ],
      "id": "D3tNg1q-M4Pi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "consistent-vegetation",
        "outputId": "8750f6c3-5a36-4680-840e-044ac4210494"
      },
      "source": [
        "sentence_generation('오늘 저녁 뭐 먹지?')"
      ],
      "id": "consistent-vegetation",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 오늘 저녁 뭐 먹지?\n",
            "출력 : 맛있는 거 드세요 .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'맛있는 거 드세요 .'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "terminal-mixer",
        "outputId": "d3dde1dd-d1fe-4cc9-d9d2-8e9bf5e9ddf6"
      },
      "source": [
        "sentence_generation('몸이 쑤셔.')"
      ],
      "id": "terminal-mixer",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 몸이 쑤셔.\n",
            "출력 : 아픈 것만큼 힘든 게 없죠 . 통증이 아직까지 남아 있으신가요 ?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'아픈 것만큼 힘든 게 없죠 . 통증이 아직까지 남아 있으신가요 ?'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "referenced-rochester",
        "outputId": "0abc0c54-a114-492a-a9b0-88bbbe7156f6"
      },
      "source": [
        "sentence_generation('졸리다.')"
      ],
      "id": "referenced-rochester",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 졸리다.\n",
            "출력 : 한 숨 주무세요 .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'한 숨 주무세요 .'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moderate-corruption",
        "outputId": "dc5a85fe-0ad1-473b-f171-71f220ff26b6"
      },
      "source": [
        "sentence_generation('공부하기 싫어.')"
      ],
      "id": "moderate-corruption",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 공부하기 싫어.\n",
            "출력 : 잠시 쉬어도 돼요 .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'잠시 쉬어도 돼요 .'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prostate-ready",
        "outputId": "2dbaf30e-b8e2-4e04-bf82-2a36cc03de60"
      },
      "source": [
        "sentence_generation('오늘은 날씨가 좋네!')"
      ],
      "id": "prostate-ready",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 오늘은 날씨가 좋네!\n",
            "출력 : 같은 하늘 아래 어딘가에 .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'같은 하늘 아래 어딘가에 .'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saving-tomato",
        "outputId": "9ebe63cd-f071-4662-95d9-bda5cd964baf"
      },
      "source": [
        "sentence_generation('오늘 저녁에 뭐 할까?')"
      ],
      "id": "saving-tomato",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 오늘 저녁에 뭐 할까?\n",
            "출력 : 쉽게 준비하는방법이 있는지 검색해서 준비해 보세요 .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'쉽게 준비하는방법이 있는지 검색해서 준비해 보세요 .'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "surgical-champagne",
        "outputId": "edd99519-71c3-4722-dd1f-52d37963167c"
      },
      "source": [
        "sentence_generation('안녕! 좋은 아침이야!')"
      ],
      "id": "surgical-champagne",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 안녕! 좋은 아침이야!\n",
            "출력 : 안녕 .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'안녕 .'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advisory-penalty",
        "outputId": "fff71388-7cbc-4f81-cf8d-e363f9238ca6"
      },
      "source": [
        "sentence_generation('나는 정민지야.')"
      ],
      "id": "advisory-penalty",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 나는 정민지야.\n",
            "출력 : 저도 궁금하네요 .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'저도 궁금하네요 .'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL-RtKtuEQIn"
      },
      "source": [
        "## 5. KoGPT2를 활용해 챗봇 구현"
      ],
      "id": "FL-RtKtuEQIn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "green-bahrain"
      },
      "source": [
        "# https://github.com/haven-jeon/KoGPT2-chatbot 의 깃헙 코드를 참고 및 가져왔습니다.\n",
        "# 위 링크 코드를 가져와서 pretrained ko-gpt2를 활용해 챗봇을 구현할 수 있었다.\n",
        "\n",
        "!pip3 install mxnet-cu102\n",
        "!pip3 install gluonnlp sentencepiece pandas torch transformers pytorch_lightning \n",
        "\n",
        "!git clone https://github.com/SKT-AI/KoGPT2.git\n",
        "%cd KoGPT2\n",
        "!pip install ."
      ],
      "id": "green-bahrain",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdVM913y-1Yt",
        "outputId": "8c412154-8a3d-4aed-aee2-80dab23c9365"
      },
      "source": [
        "!git clone --recurse-submodules https://github.com/haven-jeon/KoGPT2-chatbot.git"
      ],
      "id": "rdVM913y-1Yt",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KoGPT2-chatbot'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/15)\u001b[K\rremote: Counting objects:  13% (2/15)\u001b[K\rremote: Counting objects:  20% (3/15)\u001b[K\rremote: Counting objects:  26% (4/15)\u001b[K\rremote: Counting objects:  33% (5/15)\u001b[K\rremote: Counting objects:  40% (6/15)\u001b[K\rremote: Counting objects:  46% (7/15)\u001b[K\rremote: Counting objects:  53% (8/15)\u001b[K\rremote: Counting objects:  60% (9/15)\u001b[K\rremote: Counting objects:  66% (10/15)\u001b[K\rremote: Counting objects:  73% (11/15)\u001b[K\rremote: Counting objects:  80% (12/15)\u001b[K\rremote: Counting objects:  86% (13/15)\u001b[K\rremote: Counting objects:  93% (14/15)\u001b[K\rremote: Counting objects: 100% (15/15)\u001b[K\rremote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 100 (delta 6), reused 8 (delta 2), pack-reused 85\u001b[K\n",
            "Receiving objects: 100% (100/100), 115.32 KiB | 2.31 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n",
            "Submodule 'Chatbot_data' (https://github.com/haven-jeon/Chatbot_data.git) registered for path 'Chatbot_data'\n",
            "Cloning into '/content/KoGPT2/KoGPT2-chatbot/Chatbot_data'...\n",
            "remote: Enumerating objects: 24, done.        \n",
            "remote: Counting objects: 100% (4/4), done.        \n",
            "remote: Compressing objects: 100% (4/4), done.        \n",
            "remote: Total 24 (delta 0), reused 3 (delta 0), pack-reused 20        \n",
            "Submodule path 'Chatbot_data': checked out '235fac5aea3badab22743f7048afe936cf72f822'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "substantial-craft",
        "outputId": "ef151ca6-dc11-4cc7-e693-e72675628bec"
      },
      "source": [
        "# 앞에서 만들어준 데이터셋(챗봇 데이터셋 + 심리상담 데이터셋)을 훈련에 활용하기 위해 저장해준다.\n",
        "chat.to_csv('/content/KoGPT2/KoGPT2-chatbot/Chatbot_data/ChatbotData.csv', header=True, index=False)\n",
        "\n",
        "%cd /content/KoGPT2/KoGPT2-chatbot\n",
        "!pip install -r requirements.txt\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --train --gpus 1 --max_epochs 2  # 훈련 코드"
      ],
      "id": "substantial-craft",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KoGPT2/KoGPT2-chatbot\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.1.5)\n",
            "Collecting pytorch_lightning==1.2.10\n",
            "  Downloading pytorch_lightning-1.2.10-py3-none-any.whl (841 kB)\n",
            "\u001b[K     |████████████████████████████████| 841 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.9.0+cu102)\n",
            "Collecting transformers==4.5.1\n",
            "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 36.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (4.62.0)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (2.6.0)\n",
            "Collecting torchmetrics==0.2.0\n",
            "  Downloading torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 42.1 MB/s \n",
            "\u001b[?25hCollecting PyYAML!=5.4.*,>=5.1\n",
            "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.18.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (2021.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (4.6.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 3)) (3.7.4.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.7.4.post0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.34.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.4.5)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.39.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.3.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.37.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (21.2.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 4)) (7.1.2)\n",
            "Building wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44635 sha256=ffa7c3fb9ddddd30df002799bfb2180dd25068eef16ba2667d4b7a73d87e5688\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
            "Successfully built PyYAML\n",
            "Installing collected packages: torchmetrics, PyYAML, transformers, pytorch-lightning\n",
            "  Attempting uninstall: torchmetrics\n",
            "    Found existing installation: torchmetrics 0.5.1\n",
            "    Uninstalling torchmetrics-0.5.1:\n",
            "      Successfully uninstalled torchmetrics-0.5.1\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 5.4.1\n",
            "    Uninstalling PyYAML-5.4.1:\n",
            "      Successfully uninstalled PyYAML-5.4.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.10.0\n",
            "    Uninstalling transformers-4.10.0:\n",
            "      Successfully uninstalled transformers-4.10.0\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 1.4.5\n",
            "    Uninstalling pytorch-lightning-1.4.5:\n",
            "      Successfully uninstalled pytorch-lightning-1.4.5\n",
            "Successfully installed PyYAML-5.3.1 pytorch-lightning-1.2.10 torchmetrics-0.2.0 transformers-4.5.1\n",
            "Downloading: 100% 2.83M/2.83M [00:00<00:00, 14.9MB/s]\n",
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=96, benchmark=False, chat=False, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=5e-05, max_epochs=2, max_len=32, max_steps=None, min_epochs=None, min_steps=None, model_params='model_chp/model_-last.ckpt', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sentiment='0', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tpu_cores=None, track_grad_norm=-1, train=True, truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "INFO:filelock:Lock 139672959485520 acquired on /root/.cache/huggingface/transformers/13bb826cf24517d7849a701e02452715a67c5e560142be3d4735442b2a545809.6b384eec6effdd44287f67715cd55bd0dff2cf846d843b932b43ba7b632b8b1e.lock\n",
            "Downloading: 100% 1.00k/1.00k [00:00<00:00, 954kB/s]\n",
            "INFO:filelock:Lock 139672959485520 released on /root/.cache/huggingface/transformers/13bb826cf24517d7849a701e02452715a67c5e560142be3d4735442b2a545809.6b384eec6effdd44287f67715cd55bd0dff2cf846d843b932b43ba7b632b8b1e.lock\n",
            "INFO:filelock:Lock 139672966200720 acquired on /root/.cache/huggingface/transformers/495b405e3742953dbcc56685d1560fa02a2d86fc50b891868990a4471b06c934.4ebf112d34c2c8fc657866680005d92d21859c52c0ef5e941fa640129b2f8f88.lock\n",
            "Downloading: 100% 513M/513M [00:14<00:00, 36.0MB/s]\n",
            "INFO:filelock:Lock 139672966200720 released on /root/.cache/huggingface/transformers/495b405e3742953dbcc56685d1560fa02a2d86fc50b891868990a4471b06c934.4ebf112d34c2c8fc657866680005d92d21859c52c0ef5e941fa640129b2f8f88.lock\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | kogpt2        | GPT2LMHeadModel  | 125 M \n",
            "1 | loss_function | CrossEntropyLoss | 0     \n",
            "---------------------------------------------------\n",
            "125 M     Trainable params\n",
            "0         Non-trainable params\n",
            "125 M     Total params\n",
            "500.656   Total estimated model params size (MB)\n",
            "Epoch 0:   0% 0/124 [00:00<?, ?it/s] INFO:root:contexts : 이별을 받아드리질 못하고있네\n",
            "INFO:root:toked ctx: ['<usr>', '▁이별을', '▁받아', '드리', '질', '▁못하고', '있', '네', '<unused1>', '▁1']\n",
            "INFO:root:response : 너무 갑작스러웠을테니까요.\n",
            "INFO:root:toked response : ['<sys>', '▁너무', '▁갑작', '스러', '웠', '을', '테', '니까', '요.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁너무', '▁갑작', '스러', '웠', '을', '테', '니까', '요.', '</s>']\n",
            "INFO:root:contexts : 내 사랑은 그저 집착일 뿐이다.\n",
            "INFO:root:toked ctx: ['<usr>', '▁내', '▁사랑은', '▁그저', '▁집착', '일', '▁뿐', '이다.', '<unused1>', '▁1']\n",
            "INFO:root:response : 사랑은 사랑이에요.\n",
            "INFO:root:toked response : ['<sys>', '▁사랑은', '▁사랑', '이에', '요.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁사랑은', '▁사랑', '이에', '요.', '</s>']\n",
            "Epoch 0: 100% 124/124 [04:38<00:00,  2.24s/it, loss=28.4, v_num=0]Epoch 0, global step 123: train_loss reached 34.86535 (best 34.86535), saving model to \"/content/KoGPT2/KoGPT2-chatbot/model_chp/model_-epoch=00-train_loss=34.87.ckpt\" as top 1\n",
            "tcmalloc: large alloc 1180934144 bytes == 0x55b25a13a000 @  0x7f08e324b615 0x55b1c8a5e02c 0x55b1c8b3e17a 0x55b1c8a64a7c 0x7f08c910d5d4 0x7f08c9115cd4 0x7f08c90ea7e0 0x7f08b73ae665 0x7f08b73a9f9e 0x7f08b73af13a 0x7f08c90ead2e 0x7f08c8b80b88 0x55b1c8a61bf8 0x55b1c8ad56f2 0x55b1c8ad0235 0x55b1c8a6273a 0x55b1c8ad0b0e 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad4f40 0x55b1c8a6265a 0x55b1c8ad0b0e 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad0d67 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad4f40 0x55b1c8a6265a 0x55b1c8ad0d67 0x55b1c8a6265a\n",
            "tcmalloc: large alloc 1476173824 bytes == 0x55b2a1976000 @  0x7f08e324b615 0x55b1c8a5e02c 0x55b1c8b3e17a 0x55b1c8a64a7c 0x7f08c910d5d4 0x7f08c9115cd4 0x7f08c90ea7e0 0x7f08b73ae665 0x7f08b73a9f9e 0x7f08b73af13a 0x7f08c90ead2e 0x7f08c8b80b88 0x55b1c8a61bf8 0x55b1c8ad56f2 0x55b1c8ad0235 0x55b1c8a6273a 0x55b1c8ad0b0e 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad4f40 0x55b1c8a6265a 0x55b1c8ad0b0e 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad0d67 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad4f40 0x55b1c8a6265a 0x55b1c8ad0d67 0x55b1c8a6265a\n",
            "tcmalloc: large alloc 1845223424 bytes == 0x55b2fab42000 @  0x7f08e324b615 0x55b1c8a5e02c 0x55b1c8b3e17a 0x55b1c8a64a7c 0x7f08c910d5d4 0x7f08c9115cd4 0x7f08c90ea7e0 0x7f08b73ae665 0x7f08b73a9f9e 0x7f08b73af13a 0x7f08c90ead2e 0x7f08c8b80b88 0x55b1c8a61bf8 0x55b1c8ad56f2 0x55b1c8ad0235 0x55b1c8a6273a 0x55b1c8ad0b0e 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad4f40 0x55b1c8a6265a 0x55b1c8ad0b0e 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad0d67 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad4f40 0x55b1c8a6265a 0x55b1c8ad0d67 0x55b1c8a6265a\n",
            "Epoch 0: 100% 124/124 [04:50<00:00,  2.35s/it, loss=28.4, v_num=0]tcmalloc: large alloc 1845223424 bytes == 0x55b2f873e000 @  0x7f08e324b615 0x55b1c8a5e02c 0x55b1c8b3e17a 0x55b1c8a64a7c 0x7f08c910d5d4 0x7f08c9115cd4 0x7f08c90ea7e0 0x7f08b73ae665 0x7f08b73a9f9e 0x7f08b73af13a 0x7f08c90ead2e 0x7f08c8b80b88 0x55b1c8a61bf8 0x55b1c8ad56f2 0x55b1c8ad0235 0x55b1c8a6273a 0x55b1c8ad0b0e 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad4f40 0x55b1c8a6265a 0x55b1c8ad0b0e 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad0d67 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad4f40 0x55b1c8a6265a 0x55b1c8ad0d67 0x55b1c8a6265a\n",
            "Epoch 1:   0% 0/124 [00:00<?, ?it/s, loss=28.4, v_num=0]INFO:root:contexts : 자다가 침대에서 떨어졌어\n",
            "INFO:root:toked ctx: ['<usr>', '▁자', '다가', '▁침', '대에서', '▁떨어', '졌', '어', '<unused1>', '▁0']\n",
            "INFO:root:response : 조심하세요.\n",
            "INFO:root:toked response : ['<sys>', '▁조심', '하', '세', '요.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁조심', '하', '세', '요.', '</s>']\n",
            "INFO:root:contexts : 하루하루 너무 힘드네\n",
            "INFO:root:toked ctx: ['<usr>', '▁하루', '하루', '▁너무', '▁힘', '드', '네', '<unused1>', '▁1']\n",
            "INFO:root:response : 다른사람에게 기대보는 건 어떨까요.\n",
            "INFO:root:toked response : ['<sys>', '▁다른', '사람', '에게', '▁기대', '보는', '▁건', '▁어', '떨', '까', '요.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁다른', '사람', '에게', '▁기대', '보는', '▁건', '▁어', '떨', '까', '요.', '</s>']\n",
            "Epoch 1: 100% 124/124 [04:40<00:00,  2.26s/it, loss=27.5, v_num=0]Epoch 1, global step 247: train_loss reached 32.00912 (best 32.00912), saving model to \"/content/KoGPT2/KoGPT2-chatbot/model_chp/model_-epoch=01-train_loss=32.01.ckpt\" as top 1\n",
            "tcmalloc: large alloc 1845223424 bytes == 0x55b2f873e000 @  0x7f08e324b615 0x55b1c8a5e02c 0x55b1c8b3e17a 0x55b1c8a64a7c 0x7f08c910d5d4 0x7f08c9115cd4 0x7f08c90ea7e0 0x7f08b73ae665 0x7f08b73a9f9e 0x7f08b73af13a 0x7f08c90ead2e 0x7f08c8b80b88 0x55b1c8a61bf8 0x55b1c8ad56f2 0x55b1c8ad0235 0x55b1c8a6273a 0x55b1c8ad0b0e 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad4f40 0x55b1c8a6265a 0x55b1c8ad0b0e 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad0d67 0x55b1c8acfc35 0x55b1c8a6273a 0x55b1c8ad4f40 0x55b1c8a6265a 0x55b1c8ad0d67 0x55b1c8a6265a\n",
            "Epoch 1: 100% 124/124 [04:53<00:00,  2.37s/it, loss=27.5, v_num=0]Saving latest checkpoint...\n",
            "Epoch 1: 100% 124/124 [05:32<00:00,  2.68s/it, loss=27.5, v_num=0]\n",
            "INFO:root:best model path /content/KoGPT2/KoGPT2-chatbot/model_chp/model_-epoch=01-train_loss=32.01.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "animal-development",
        "outputId": "eb8c7710-2ee4-430a-a1d0-9bec4a630245"
      },
      "source": [
        "# 심시미와 대화하기\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --gpus 1 --chat"
      ],
      "id": "animal-development",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=96, benchmark=False, chat=True, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=5e-05, max_epochs=None, max_len=32, max_steps=None, min_epochs=None, min_steps=None, model_params='model_chp/model_-last.ckpt', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sentiment='0', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tpu_cores=None, track_grad_norm=-1, train=False, truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "user > 안녕~ 나는 민지야. \n",
            "Simsimi > 안녕하세요.\n",
            "user > 너는 누구니?\n",
            "Simsimi > 저는 위로봇입니다.\n",
            "user > 나는 오늘 운동을 했어.\n",
            "Simsimi > 운동으로 땀을 내보세요.\n",
            "user > 피곤한 하루다.\n",
            "Simsimi > 오늘도 고생이 많으시네요.\n",
            "user > 고마워.\n",
            "Simsimi > 고마운만큼 보답을 바라지 마세요.\n",
            "user > 내일은 뭐할까?\n",
            "Simsimi > 저랑 놀아요.\n",
            "user > 그래!\n",
            "Simsimi > 저도 그래요.\n",
            "user > 심심미야 심심하다.\n",
            "Simsimi > 저랑 놀아요.\n",
            "user > 뭐하고 놀래?\n",
            "Simsimi > 저랑 놀아요.\n",
            "user > 공부하기 싫다.\n",
            "Simsimi > 공부는 끝이 없죠.\n",
            "user > 잘자. 심심아.\n",
            "Simsimi > 저도 놀랐어요.\n",
            "user > quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "earned-conditions"
      },
      "source": [
        "## 6. 회고\n",
        "- 챗봇이 출력을 해주는 문장을 보면 어느 정도 말도 되고, 재미있는 것 같다. 물론 뜬금없는 대답을 하기도 한다.\n",
        "- 1. 먼저 챗봇 구현에는 트랜스포머 모델을 사용했으며, 인코더 레이어는 셀프어텐션(멀티헤드어텐션)과 피드포워드로 구성되고, 디코더 레이어는 멀티헤드어텐션, 인코더-디코더 어텐션, 피드포워드로 구성된다.\n",
        "- 2. 데이터의 다양성 : 챗봇 데이터에 더해 AI 허브에서 제공하고 있는 정신상담 데이터셋을 사용했다. 정신상담 데이터셋은 nan 값을 제거하니 데이터셋이 작아져서 효과도 작게 나타났다. 기록은 하지 못했지만 데이터셋 추가 전과 비교하면 조금 더 위로를 길게 잘하게 되었다. 이번 실습에서 더 풍부한 데이터셋을 시도해보는 의의가 있었다.  \n",
        "특히 \"몸이 쑤셔\"라고 말했을 때, songys 데이터셋만 사용했을 때는 \"걱정되네요.\"라고 했는데 데이터셋을 추가하니 이렇게 답했다.  \n",
        ">입력 : 몸이 쑤셔.  \n",
        "출력 : 아픈 것만큼 힘든 게 없죠 . 통증이 아직까지 남아 있으신가요 ?\n",
        "- 3. 파라미터 설정의 중요성: 인코더, 디코더 레이어를 6개 쌓으니 오히려 챗봇이 이상한 말을 했다. 레이어를 3-4개 쌓을 때가 챗봇이 잘 작동했다.\n",
        "- 추가적으로 pretrained 언어 모델인 koGPT2를 활용해서 챗봇을 구현한 [링크](https://github.com/haven-jeon/Chatbot_data.git)를 가져와서 챗봇과 대화도 해보았다. 훈련할 때는 앞에서 내가 만들어준 (챗봇 데이터 + 심리상담 데이터)를 활용했다. SIMSIMI는 꽤 대화가 잘 통하는 것 같다."
      ],
      "id": "earned-conditions"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS1dR2nfED4S"
      },
      "source": [
        ""
      ],
      "id": "yS1dR2nfED4S",
      "execution_count": null,
      "outputs": []
    }
  ]
}